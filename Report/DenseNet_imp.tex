\section{Densely Connected Convolutional Networks - DenseNet}

This section will elaborate on the implemented DenseNet. The implementation is done in python 2.7 and Jupyter. The implementation of the DenseNet is from the github repository\footnote{\url{https://github.com/awjuliani/TF-Tutorials}}. Article \citep{DENSE} has been used as the basis for the implementation of the DenseNet.

The DenseNet in this project consist of 12 layers where 5 layers are dense blocks, and 7 layers are convolutional layers. At the end of the DenseNet a flatten layer and a softmax layer are applied. Adam is used as activation function.

The creation of the 7 convolutional layers and 5 dense block's can be seen in listing \ref{lst:denseconvo}.

\begin{lstlisting}[language=Python, label=lst:denseconvo, caption=for loop which creates the dense block's mixed with the convolutional layers]
layer1 = slim.conv2d(input_layer,64,[3,3],
	normalizer_fn=slim.batch_norm,scope='conv_'+str(0))
for i in range(5):
	layer1 = denseBlock(layer1,i,units_between_stride)
	layer1 = slim.conv2d(layer1,64,[3,3],stride=[2,2],
		normalizer_fn=slim.batch_norm,scope='conv_s_'+str(i))

top = slim.conv2d(layer1,10,[3,3],
	normalizer_fn=slim.batch_norm,activation_fn=None,scope='conv_top')
\end{lstlisting}

The convolutional layers has a filter size of 3 by 3, and a stride with 2 by 2. 

The dense block's consist of 6 convolutional layers which are connected in a direct connection to all subsequent layers and have a filter size of 3 by 3. The code for creating the dense block's can be seen in listing \ref{lst:densenetblock}. The last convolutional layer is the top layer which is a fully connected layer.

\begin{lstlisting}[language=Python, label=lst:densenetblock, caption=DenseNet Block function]
def denseBlock(input_layer,i,j):
	with tf.variable_scope("dense_unit"+str(i)):
	nodes = []
	a = slim.conv2d(input_layer,64,[3,3],normalizer_fn=slim.batch_norm)
	nodes.append(a)
	for z in range(j):
		b = slim.conv2d(tf.concat(nodes,3),64,[3,3],
				normalizer_fn=slim.batch_norm)
		nodes.append(b)
	return b
\end{lstlisting}

Listing \ref{lst:softmax} shows the last layer which is the softmax layer. This layer is appended to \emph{top} which contains all the previous layers.

\begin{lstlisting}[language=Python, label=lst:softmax, caption=Softmax layer appended to the end of the DenseNet]
output = slim.layers.softmax(slim.layers.flatten(top))
\end{lstlisting}

When all the layers are prepared the DenseNet is ready to be used with TensorFlow, and the training can commence.