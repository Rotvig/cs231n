\section{Deep Neural Net - RegularNet}
composed exclusively of regular and strided convolutional layers. While this architecture works well for relatively shallow networks, it becomes increasingly more difficult to train as the network depth increases.

Regular networks are a combination of the two standard forms of neural networks, the fully connected neural network and the convolution neural network.

\subsection{Fully Connected Neural Networks}
A neural network consist of an input layer, some hidden layers and an output layer. The input layer is an image, and it is targeted for feature extraction and classification in the hidden layers and output layer. Every pixel in the image is connected to every neuron in the first hidden layer of the neural network.    
Each hidden layer is an array of neurons, with each neuron normally consisting of a weight, some activation function and a regulation function. Each pixel value is so weighted, activated in an activation function and regularized. Most often ReLU or Leaky ReLU are used as activation function to zero out negative values. The activation function provides a non-linear relationship within the data points to provide better feature extractions. Each neuron in the second hidden layer is supplied with the output of all the neurons in the previous layer and the procedure is repeated until the last hidden layer. The output layer consist of a loss function which is often either a Softmax loss function or a support vector machine loss function. The amount of loss functions in the output layer is equal to the amount of classification categories the image can be classified as.

An image is forwarded through the neural network, and the loss functions provided the misclassification percentage of the image. This error, or loss in accuracy, is send back through the neural network and a gradient for each neuron is found. This process is repeated for, normally, an integer amount of images, batchsize, and for each backward propagation the gradient is saved. After a batch of images all the gradients saved for an individual neuron are average and from this value the neural network response to the back propagation. The weights are updated depending on the averaged gradients and this procedure can be done in different ways. The most modern update method is called ADAM, and it is trying to reduced the loss result by changing the weights regarding the gradients. ADAM will decided the amount of change the weights must have in relation to the gradients, while the regularization step decides the relationship of the change between the weights. Say ADAM defines the maximum change to a single weight, while the regularization defines the distribution of change over all weights related to the maximum defined change.

\myFigure{Regular}{Fully Connected Neural Network Architecture \citep{RegularNET_FCNN}}{fig:RegularFCNN}{0.5}
\FloatBarrier

\subsection{Convolutional Neural Networks}


%Troels