{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the CIFAR10 dataset, go here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "The training data is stored in 5 separate files, and we will alternate between them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def ConvertImages(raw):\n",
    "    \"\"\"\n",
    "    Convert images from the CIFAR-10 format and\n",
    "    return a 4-dim array with shape: [image_number, height, width, channel]\n",
    "    where the pixels are floats between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the raw images from the data-files to floating-points.\n",
    "    raw_float = np.array(raw, dtype=float) / 255.0\n",
    "\n",
    "    # Reshape the array to 4-dimensions.\n",
    "    images = raw_float.reshape([-1, 3, 32, 32])\n",
    "\n",
    "    # Reorder the indices of the array.\n",
    "    images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentCifar = 1\n",
    "cifar = unpickle('./cifar10/data_batch_1')\n",
    "cifarT = unpickle('./cifar10/test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_layers = 25 #Specify how deep we want our network\n",
    "units_between_stride = total_layers / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet\n",
    "An implementation of a Dense Network as described in [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denseBlock(input_layer,i,j):\n",
    "    with tf.variable_scope(\"dense_unit\"+str(i)):\n",
    "        nodes = []\n",
    "        a = slim.conv2d(input_layer,64,[3,3],normalizer_fn=slim.batch_norm)\n",
    "        nodes.append(a)\n",
    "        for z in range(j):\n",
    "            b = slim.conv2d(tf.concat(nodes,3),64,[3,3],normalizer_fn=slim.batch_norm)\n",
    "            nodes.append(b)\n",
    "        return b\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_layer = tf.placeholder(shape=[None,32,32,3],dtype=tf.float32,name='input')\n",
    "label_layer = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "label_oh = slim.layers.one_hot_encoding(label_layer,10)\n",
    "\n",
    "layer1 = slim.conv2d(input_layer,64,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(0))\n",
    "for i in range(5):\n",
    "    layer1 = denseBlock(layer1,i,units_between_stride)\n",
    "    layer1 = slim.conv2d(layer1,64,[3,3],stride=[2,2],normalizer_fn=slim.batch_norm,scope='conv_s_'+str(i))\n",
    "    \n",
    "top = slim.conv2d(layer1,10,[3,3],normalizer_fn=slim.batch_norm,activation_fn=None,scope='conv_top')\n",
    "\n",
    "output = slim.layers.softmax(slim.layers.flatten(top))\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(label_oh * tf.log(output) + 1e-10, axis=[1]))\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "update = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the network graph\n",
    "We can call the Tensorflow Board to provide a graphical representation of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched CIFAR set to 2\n",
      "Step: 0 Loss: 2.9222 Accuracy: 0.078125\n",
      "Test set accuracy: 0.114\n",
      "Step: 10 Loss: 2.55995 Accuracy: 0.078125\n",
      "Step: 20 Loss: 2.25432 Accuracy: 0.15625\n",
      "Step: 30 Loss: 2.27495 Accuracy: 0.1875\n",
      "Step: 40 Loss: 2.14674 Accuracy: 0.25\n",
      "Step: 50 Loss: 1.97689 Accuracy: 0.296875\n",
      "Step: 60 Loss: 2.00566 Accuracy: 0.296875\n",
      "Step: 70 Loss: 1.85913 Accuracy: 0.390625\n",
      "Step: 80 Loss: 2.0484 Accuracy: 0.203125\n",
      "Step: 90 Loss: 1.75845 Accuracy: 0.34375\n",
      "Step: 100 Loss: 1.74863 Accuracy: 0.34375\n",
      "Test set accuracy: 0.33\n",
      "Step: 110 Loss: 1.84953 Accuracy: 0.28125\n",
      "Step: 120 Loss: 1.6891 Accuracy: 0.40625\n",
      "Step: 130 Loss: 1.94572 Accuracy: 0.3125\n",
      "Step: 140 Loss: 1.64267 Accuracy: 0.390625\n",
      "Step: 150 Loss: 1.77556 Accuracy: 0.265625\n",
      "Switched CIFAR set to 3\n",
      "Step: 160 Loss: 1.74516 Accuracy: 0.375\n",
      "Step: 170 Loss: 1.51322 Accuracy: 0.390625\n",
      "Step: 180 Loss: 1.70762 Accuracy: 0.296875\n",
      "Step: 190 Loss: 1.5873 Accuracy: 0.46875\n",
      "Step: 200 Loss: 1.42377 Accuracy: 0.53125\n",
      "Test set accuracy: 0.442\n",
      "Step: 210 Loss: 1.67071 Accuracy: 0.421875\n",
      "Step: 220 Loss: 1.52874 Accuracy: 0.4375\n",
      "Step: 230 Loss: 1.51645 Accuracy: 0.5\n",
      "Step: 240 Loss: 1.76664 Accuracy: 0.390625\n",
      "Step: 250 Loss: 1.60402 Accuracy: 0.359375\n",
      "Step: 260 Loss: 1.60799 Accuracy: 0.40625\n",
      "Step: 270 Loss: 1.59099 Accuracy: 0.40625\n",
      "Step: 280 Loss: 1.43942 Accuracy: 0.625\n",
      "Step: 290 Loss: 1.47686 Accuracy: 0.46875\n",
      "Step: 300 Loss: 1.46524 Accuracy: 0.515625\n",
      "Test set accuracy: 0.41\n",
      "Step: 310 Loss: 1.38652 Accuracy: 0.53125\n",
      "Switched CIFAR set to 4\n",
      "Step: 320 Loss: 1.55733 Accuracy: 0.4375\n",
      "Step: 330 Loss: 1.53479 Accuracy: 0.46875\n",
      "Step: 340 Loss: 1.57082 Accuracy: 0.421875\n",
      "Step: 350 Loss: 1.62474 Accuracy: 0.421875\n",
      "Step: 360 Loss: 1.59259 Accuracy: 0.46875\n",
      "Step: 370 Loss: 1.57565 Accuracy: 0.390625\n",
      "Step: 380 Loss: 1.49572 Accuracy: 0.453125\n",
      "Step: 390 Loss: 1.41375 Accuracy: 0.515625\n",
      "Step: 400 Loss: 1.45834 Accuracy: 0.4375\n",
      "Test set accuracy: 0.498\n",
      "Step: 410 Loss: 1.42121 Accuracy: 0.53125\n",
      "Step: 420 Loss: 1.49053 Accuracy: 0.453125\n",
      "Step: 430 Loss: 1.68239 Accuracy: 0.40625\n",
      "Step: 440 Loss: 1.25863 Accuracy: 0.59375\n",
      "Step: 450 Loss: 1.61635 Accuracy: 0.375\n",
      "Step: 460 Loss: 1.50117 Accuracy: 0.484375\n",
      "Switched CIFAR set to 5\n",
      "Step: 470 Loss: 1.49864 Accuracy: 0.46875\n",
      "Step: 480 Loss: 1.30188 Accuracy: 0.546875\n",
      "Step: 490 Loss: 1.3287 Accuracy: 0.53125\n",
      "Step: 500 Loss: 1.5007 Accuracy: 0.46875\n",
      "Test set accuracy: 0.502\n",
      "Step: 510 Loss: 1.28814 Accuracy: 0.515625\n",
      "Step: 520 Loss: 1.38722 Accuracy: 0.5\n",
      "Step: 530 Loss: 1.30455 Accuracy: 0.65625\n",
      "Step: 540 Loss: 1.30344 Accuracy: 0.578125\n",
      "Step: 550 Loss: 1.28202 Accuracy: 0.515625\n",
      "Step: 560 Loss: 1.36221 Accuracy: 0.578125\n",
      "Step: 570 Loss: 1.38473 Accuracy: 0.46875\n",
      "Step: 580 Loss: 1.04224 Accuracy: 0.6875\n",
      "Step: 590 Loss: 1.20018 Accuracy: 0.640625\n",
      "Step: 600 Loss: 1.34352 Accuracy: 0.53125\n",
      "Test set accuracy: 0.514\n",
      "Step: 610 Loss: 1.43572 Accuracy: 0.4375\n",
      "Step: 620 Loss: 1.13594 Accuracy: 0.71875\n",
      "Switched CIFAR set to 1\n",
      "Step: 630 Loss: 1.14206 Accuracy: 0.671875\n",
      "Step: 640 Loss: 1.2254 Accuracy: 0.609375\n",
      "Step: 650 Loss: 1.24993 Accuracy: 0.59375\n",
      "Step: 660 Loss: 1.4054 Accuracy: 0.515625\n",
      "Step: 670 Loss: 1.29635 Accuracy: 0.609375\n",
      "Step: 680 Loss: 1.11485 Accuracy: 0.65625\n",
      "Step: 690 Loss: 1.30132 Accuracy: 0.546875\n",
      "Step: 700 Loss: 1.21051 Accuracy: 0.609375\n",
      "Test set accuracy: 0.496\n",
      "Step: 710 Loss: 1.18861 Accuracy: 0.609375\n",
      "Step: 720 Loss: 1.34598 Accuracy: 0.59375\n",
      "Step: 730 Loss: 1.07989 Accuracy: 0.703125\n",
      "Step: 740 Loss: 1.17823 Accuracy: 0.59375\n",
      "Step: 750 Loss: 1.24201 Accuracy: 0.609375\n",
      "Step: 760 Loss: 1.14769 Accuracy: 0.609375\n",
      "Step: 770 Loss: 1.41485 Accuracy: 0.453125\n",
      "Switched CIFAR set to 2\n",
      "Step: 780 Loss: 1.39323 Accuracy: 0.5625\n",
      "Step: 790 Loss: 1.23088 Accuracy: 0.640625\n",
      "Step: 800 Loss: 1.45276 Accuracy: 0.484375\n",
      "Test set accuracy: 0.568\n",
      "Step: 810 Loss: 1.21586 Accuracy: 0.609375\n",
      "Step: 820 Loss: 1.10753 Accuracy: 0.71875\n",
      "Step: 830 Loss: 1.26359 Accuracy: 0.53125\n",
      "Step: 840 Loss: 1.32042 Accuracy: 0.546875\n",
      "Step: 850 Loss: 1.2923 Accuracy: 0.578125\n",
      "Step: 860 Loss: 1.24206 Accuracy: 0.5625\n",
      "Step: 870 Loss: 1.25315 Accuracy: 0.578125\n",
      "Step: 880 Loss: 1.23878 Accuracy: 0.53125\n",
      "Step: 890 Loss: 1.18968 Accuracy: 0.609375\n",
      "Step: 900 Loss: 1.36311 Accuracy: 0.515625\n",
      "Test set accuracy: 0.6\n",
      "Step: 910 Loss: 1.28241 Accuracy: 0.65625\n",
      "Step: 920 Loss: 1.2205 Accuracy: 0.640625\n",
      "Step: 930 Loss: 1.06187 Accuracy: 0.671875\n",
      "Switched CIFAR set to 3\n",
      "Step: 940 Loss: 1.45352 Accuracy: 0.5\n",
      "Step: 950 Loss: 1.1855 Accuracy: 0.671875\n",
      "Step: 960 Loss: 1.32307 Accuracy: 0.515625\n",
      "Step: 970 Loss: 1.05586 Accuracy: 0.734375\n",
      "Step: 980 Loss: 1.05022 Accuracy: 0.671875\n",
      "Step: 990 Loss: 1.12793 Accuracy: 0.640625\n",
      "Step: 1000 Loss: 1.05491 Accuracy: 0.71875\n",
      "Test set accuracy: 0.632\n",
      "Step: 1010 Loss: 1.19995 Accuracy: 0.59375\n",
      "Step: 1020 Loss: 1.2101 Accuracy: 0.609375\n",
      "Step: 1030 Loss: 0.942086 Accuracy: 0.78125\n",
      "Step: 1040 Loss: 1.19551 Accuracy: 0.625\n",
      "Step: 1050 Loss: 1.20416 Accuracy: 0.640625\n",
      "Step: 1060 Loss: 1.12051 Accuracy: 0.625\n",
      "Step: 1070 Loss: 0.900654 Accuracy: 0.75\n",
      "Step: 1080 Loss: 1.06295 Accuracy: 0.609375\n",
      "Step: 1090 Loss: 1.036 Accuracy: 0.71875\n",
      "Switched CIFAR set to 4\n",
      "Step: 1100 Loss: 1.05536 Accuracy: 0.65625\n",
      "Test set accuracy: 0.592\n",
      "Step: 1110 Loss: 1.11789 Accuracy: 0.65625\n",
      "Step: 1120 Loss: 1.09977 Accuracy: 0.640625\n",
      "Step: 1130 Loss: 1.12889 Accuracy: 0.640625\n",
      "Step: 1140 Loss: 0.991547 Accuracy: 0.71875\n",
      "Step: 1150 Loss: 1.08731 Accuracy: 0.65625\n",
      "Step: 1160 Loss: 1.32354 Accuracy: 0.5625\n",
      "Step: 1170 Loss: 1.24166 Accuracy: 0.609375\n",
      "Step: 1180 Loss: 1.08893 Accuracy: 0.640625\n",
      "Step: 1190 Loss: 1.28702 Accuracy: 0.609375\n",
      "Step: 1200 Loss: 1.06594 Accuracy: 0.703125\n",
      "Test set accuracy: 0.634\n",
      "Step: 1210 Loss: 1.13733 Accuracy: 0.640625\n",
      "Step: 1220 Loss: 1.12489 Accuracy: 0.640625\n",
      "Step: 1230 Loss: 0.909682 Accuracy: 0.734375\n",
      "Step: 1240 Loss: 0.888074 Accuracy: 0.75\n",
      "Switched CIFAR set to 5\n",
      "Step: 1250 Loss: 1.11691 Accuracy: 0.65625\n",
      "Step: 1260 Loss: 1.06844 Accuracy: 0.703125\n",
      "Step: 1270 Loss: 1.17709 Accuracy: 0.65625\n",
      "Step: 1280 Loss: 1.29072 Accuracy: 0.609375\n",
      "Step: 1290 Loss: 1.23824 Accuracy: 0.578125\n",
      "Step: 1300 Loss: 1.06009 Accuracy: 0.65625\n",
      "Test set accuracy: 0.636\n",
      "Step: 1310 Loss: 1.04102 Accuracy: 0.65625\n",
      "Step: 1320 Loss: 1.20815 Accuracy: 0.65625\n",
      "Step: 1330 Loss: 1.11077 Accuracy: 0.6875\n",
      "Step: 1340 Loss: 1.17714 Accuracy: 0.609375\n",
      "Step: 1350 Loss: 0.88223 Accuracy: 0.71875\n",
      "Step: 1360 Loss: 0.903737 Accuracy: 0.75\n",
      "Step: 1370 Loss: 0.809483 Accuracy: 0.78125\n",
      "Step: 1380 Loss: 1.06183 Accuracy: 0.640625\n",
      "Step: 1390 Loss: 1.14752 Accuracy: 0.609375\n",
      "Step: 1400 Loss: 1.11208 Accuracy: 0.65625\n",
      "Test set accuracy: 0.65\n",
      "Switched CIFAR set to 1\n",
      "Step: 1410 Loss: 1.19067 Accuracy: 0.640625\n",
      "Step: 1420 Loss: 1.15403 Accuracy: 0.640625\n",
      "Step: 1430 Loss: 1.13253 Accuracy: 0.671875\n",
      "Step: 1440 Loss: 1.10996 Accuracy: 0.65625\n",
      "Step: 1450 Loss: 1.09206 Accuracy: 0.6875\n",
      "Step: 1460 Loss: 0.891242 Accuracy: 0.75\n",
      "Step: 1470 Loss: 0.977462 Accuracy: 0.71875\n",
      "Step: 1480 Loss: 1.08692 Accuracy: 0.703125\n",
      "Step: 1490 Loss: 0.954106 Accuracy: 0.75\n",
      "Step: 1500 Loss: 0.997406 Accuracy: 0.71875\n",
      "Test set accuracy: 0.672\n",
      "Step: 1510 Loss: 1.11724 Accuracy: 0.671875\n",
      "Step: 1520 Loss: 1.06452 Accuracy: 0.671875\n",
      "Step: 1530 Loss: 1.00684 Accuracy: 0.703125\n",
      "Step: 1540 Loss: 0.923138 Accuracy: 0.734375\n",
      "Step: 1550 Loss: 1.26742 Accuracy: 0.578125\n",
      "Switched CIFAR set to 2\n",
      "Step: 1560 Loss: 1.0537 Accuracy: 0.703125\n",
      "Step: 1570 Loss: 0.927213 Accuracy: 0.71875\n",
      "Step: 1580 Loss: 0.98346 Accuracy: 0.703125\n",
      "Step: 1590 Loss: 0.980305 Accuracy: 0.71875\n",
      "Step: 1600 Loss: 1.19994 Accuracy: 0.609375\n",
      "Test set accuracy: 0.7\n",
      "Step: 1610 Loss: 1.05014 Accuracy: 0.65625\n",
      "Step: 1620 Loss: 0.877844 Accuracy: 0.78125\n",
      "Step: 1630 Loss: 1.214 Accuracy: 0.578125\n",
      "Step: 1640 Loss: 1.00153 Accuracy: 0.71875\n",
      "Step: 1650 Loss: 1.01846 Accuracy: 0.6875\n",
      "Step: 1660 Loss: 0.905201 Accuracy: 0.75\n",
      "Step: 1670 Loss: 1.08678 Accuracy: 0.6875\n",
      "Step: 1680 Loss: 0.91799 Accuracy: 0.75\n",
      "Step: 1690 Loss: 0.923684 Accuracy: 0.765625\n",
      "Step: 1700 Loss: 0.867973 Accuracy: 0.765625\n",
      "Test set accuracy: 0.712\n",
      "Step: 1710 Loss: 0.762231 Accuracy: 0.8125\n",
      "Switched CIFAR set to 3\n",
      "Step: 1720 Loss: 1.27744 Accuracy: 0.578125\n",
      "Step: 1730 Loss: 1.1355 Accuracy: 0.640625\n",
      "Step: 1740 Loss: 1.16583 Accuracy: 0.609375\n",
      "Step: 1750 Loss: 1.04794 Accuracy: 0.65625\n",
      "Step: 1760 Loss: 0.924786 Accuracy: 0.71875\n",
      "Step: 1770 Loss: 1.05299 Accuracy: 0.65625\n",
      "Step: 1780 Loss: 1.0965 Accuracy: 0.640625\n",
      "Step: 1790 Loss: 0.80608 Accuracy: 0.796875\n",
      "Step: 1800 Loss: 0.912827 Accuracy: 0.765625\n",
      "Test set accuracy: 0.734\n",
      "Step: 1810 Loss: 0.9626 Accuracy: 0.75\n",
      "Step: 1820 Loss: 0.990113 Accuracy: 0.734375\n",
      "Step: 1830 Loss: 0.937869 Accuracy: 0.71875\n",
      "Step: 1840 Loss: 0.76981 Accuracy: 0.796875\n",
      "Step: 1850 Loss: 0.904234 Accuracy: 0.75\n",
      "Step: 1860 Loss: 0.851679 Accuracy: 0.765625\n",
      "Step: 1870 Loss: 0.761699 Accuracy: 0.78125\n",
      "Switched CIFAR set to 4\n",
      "Step: 1880 Loss: 0.962788 Accuracy: 0.734375\n",
      "Step: 1890 Loss: 1.06302 Accuracy: 0.640625\n",
      "Step: 1900 Loss: 1.09865 Accuracy: 0.671875\n",
      "Test set accuracy: 0.728\n",
      "Step: 1910 Loss: 0.979479 Accuracy: 0.71875\n",
      "Step: 1920 Loss: 1.07188 Accuracy: 0.703125\n",
      "Step: 1930 Loss: 0.81085 Accuracy: 0.796875\n",
      "Step: 1940 Loss: 0.890552 Accuracy: 0.765625\n",
      "Step: 1950 Loss: 0.942579 Accuracy: 0.734375\n",
      "Step: 1960 Loss: 0.835696 Accuracy: 0.78125\n",
      "Step: 1970 Loss: 0.860507 Accuracy: 0.796875\n",
      "Step: 1980 Loss: 0.941256 Accuracy: 0.75\n",
      "Step: 1990 Loss: 0.862833 Accuracy: 0.78125\n",
      "Step: 2000 Loss: 0.846787 Accuracy: 0.71875\n",
      "Test set accuracy: 0.708\n",
      "Step: 2010 Loss: 0.983488 Accuracy: 0.734375\n",
      "Step: 2020 Loss: 0.937314 Accuracy: 0.765625\n",
      "Switched CIFAR set to 5\n",
      "Step: 2030 Loss: 1.11 Accuracy: 0.703125\n",
      "Step: 2040 Loss: 0.915125 Accuracy: 0.75\n",
      "Step: 2050 Loss: 1.11783 Accuracy: 0.6875\n",
      "Step: 2060 Loss: 0.931672 Accuracy: 0.765625\n",
      "Step: 2070 Loss: 0.888663 Accuracy: 0.765625\n",
      "Step: 2080 Loss: 1.0667 Accuracy: 0.71875\n",
      "Step: 2090 Loss: 0.764171 Accuracy: 0.78125\n",
      "Step: 2100 Loss: 0.861269 Accuracy: 0.765625\n",
      "Test set accuracy: 0.706\n",
      "Step: 2110 Loss: 0.670263 Accuracy: 0.828125\n",
      "Step: 2120 Loss: 0.793248 Accuracy: 0.8125\n",
      "Step: 2130 Loss: 1.01917 Accuracy: 0.75\n",
      "Step: 2140 Loss: 0.73103 Accuracy: 0.859375\n",
      "Step: 2150 Loss: 0.834216 Accuracy: 0.765625\n",
      "Step: 2160 Loss: 0.875947 Accuracy: 0.75\n",
      "Step: 2170 Loss: 1.00731 Accuracy: 0.671875\n",
      "Step: 2180 Loss: 1.0027 Accuracy: 0.734375\n",
      "Switched CIFAR set to 1\n",
      "Step: 2190 Loss: 1.07416 Accuracy: 0.6875\n",
      "Step: 2200 Loss: 0.972672 Accuracy: 0.65625\n",
      "Test set accuracy: 0.696\n",
      "Step: 2210 Loss: 0.838591 Accuracy: 0.828125\n",
      "Step: 2220 Loss: 1.17908 Accuracy: 0.65625\n",
      "Step: 2230 Loss: 1.03336 Accuracy: 0.65625\n",
      "Step: 2240 Loss: 0.991929 Accuracy: 0.734375\n",
      "Step: 2250 Loss: 0.756443 Accuracy: 0.84375\n",
      "Step: 2260 Loss: 0.7509 Accuracy: 0.8125\n",
      "Step: 2270 Loss: 0.796785 Accuracy: 0.78125\n",
      "Step: 2280 Loss: 0.645343 Accuracy: 0.890625\n",
      "Step: 2290 Loss: 0.846984 Accuracy: 0.78125\n",
      "Step: 2300 Loss: 0.873278 Accuracy: 0.796875\n",
      "Test set accuracy: 0.736\n",
      "Step: 2310 Loss: 0.817431 Accuracy: 0.734375\n",
      "Step: 2320 Loss: 0.766916 Accuracy: 0.796875\n",
      "Step: 2330 Loss: 0.757532 Accuracy: 0.8125\n",
      "Switched CIFAR set to 2\n",
      "Step: 2340 Loss: 1.01295 Accuracy: 0.71875\n",
      "Step: 2350 Loss: 0.896023 Accuracy: 0.78125\n",
      "Step: 2360 Loss: 0.985884 Accuracy: 0.703125\n",
      "Step: 2370 Loss: 1.07509 Accuracy: 0.65625\n",
      "Step: 2380 Loss: 0.926144 Accuracy: 0.703125\n",
      "Step: 2390 Loss: 0.937072 Accuracy: 0.703125\n",
      "Step: 2400 Loss: 0.783028 Accuracy: 0.828125\n",
      "Test set accuracy: 0.734\n",
      "Step: 2410 Loss: 0.840466 Accuracy: 0.796875\n",
      "Step: 2420 Loss: 0.845763 Accuracy: 0.765625\n",
      "Step: 2430 Loss: 0.745974 Accuracy: 0.828125\n",
      "Step: 2440 Loss: 0.880102 Accuracy: 0.734375\n",
      "Step: 2450 Loss: 0.973103 Accuracy: 0.71875\n",
      "Step: 2460 Loss: 0.811218 Accuracy: 0.765625\n",
      "Step: 2470 Loss: 0.967498 Accuracy: 0.734375\n",
      "Step: 2480 Loss: 0.582267 Accuracy: 0.890625\n",
      "Step: 2490 Loss: 0.918833 Accuracy: 0.734375\n",
      "Switched CIFAR set to 3\n",
      "Step: 2500 Loss: 1.01232 Accuracy: 0.71875\n",
      "Test set accuracy: 0.74\n",
      "Step: 2510 Loss: 0.940466 Accuracy: 0.71875\n",
      "Step: 2520 Loss: 0.914687 Accuracy: 0.734375\n",
      "Step: 2530 Loss: 0.789624 Accuracy: 0.796875\n",
      "Step: 2540 Loss: 0.851827 Accuracy: 0.765625\n",
      "Step: 2550 Loss: 0.736484 Accuracy: 0.8125\n",
      "Step: 2560 Loss: 0.859326 Accuracy: 0.828125\n",
      "Step: 2570 Loss: 0.76514 Accuracy: 0.796875\n",
      "Step: 2580 Loss: 0.778196 Accuracy: 0.828125\n",
      "Step: 2590 Loss: 0.865214 Accuracy: 0.734375\n",
      "Step: 2600 Loss: 0.743496 Accuracy: 0.84375\n",
      "Test set accuracy: 0.736\n",
      "Step: 2610 Loss: 0.881751 Accuracy: 0.734375\n",
      "Step: 2620 Loss: 0.776759 Accuracy: 0.796875\n",
      "Step: 2630 Loss: 0.641344 Accuracy: 0.84375\n",
      "Step: 2640 Loss: 0.690351 Accuracy: 0.859375\n",
      "Step: 2650 Loss: 0.817723 Accuracy: 0.8125\n",
      "Switched CIFAR set to 4\n",
      "Step: 2660 Loss: 1.11178 Accuracy: 0.6875\n",
      "Step: 2670 Loss: 0.981353 Accuracy: 0.734375\n",
      "Step: 2680 Loss: 0.825606 Accuracy: 0.734375\n",
      "Step: 2690 Loss: 0.966607 Accuracy: 0.703125\n",
      "Step: 2700 Loss: 0.981987 Accuracy: 0.71875\n",
      "Test set accuracy: 0.706\n",
      "Step: 2710 Loss: 1.03464 Accuracy: 0.671875\n",
      "Step: 2720 Loss: 0.840998 Accuracy: 0.796875\n",
      "Step: 2730 Loss: 0.93287 Accuracy: 0.703125\n",
      "Step: 2740 Loss: 0.852485 Accuracy: 0.71875\n",
      "Step: 2750 Loss: 0.826649 Accuracy: 0.78125\n",
      "Step: 2760 Loss: 0.736935 Accuracy: 0.828125\n",
      "Step: 2770 Loss: 0.650514 Accuracy: 0.890625\n",
      "Step: 2780 Loss: 0.868013 Accuracy: 0.734375\n",
      "Step: 2790 Loss: 0.72898 Accuracy: 0.796875\n",
      "Step: 2800 Loss: 0.917256 Accuracy: 0.703125\n",
      "Test set accuracy: 0.738\n",
      "Switched CIFAR set to 5\n",
      "Step: 2810 Loss: 0.743896 Accuracy: 0.8125\n",
      "Step: 2820 Loss: 1.11531 Accuracy: 0.640625\n",
      "Step: 2830 Loss: 0.749795 Accuracy: 0.796875\n",
      "Step: 2840 Loss: 0.673264 Accuracy: 0.890625\n",
      "Step: 2850 Loss: 0.850333 Accuracy: 0.71875\n",
      "Step: 2860 Loss: 0.932074 Accuracy: 0.703125\n",
      "Step: 2870 Loss: 0.766914 Accuracy: 0.8125\n",
      "Step: 2880 Loss: 0.919133 Accuracy: 0.734375\n",
      "Step: 2890 Loss: 1.00084 Accuracy: 0.703125\n",
      "Step: 2900 Loss: 0.626978 Accuracy: 0.875\n",
      "Test set accuracy: 0.782\n",
      "Step: 2910 Loss: 0.808254 Accuracy: 0.78125\n",
      "Step: 2920 Loss: 0.99618 Accuracy: 0.703125\n",
      "Step: 2930 Loss: 0.765762 Accuracy: 0.796875\n",
      "Step: 2940 Loss: 0.666641 Accuracy: 0.828125\n",
      "Step: 2950 Loss: 0.792847 Accuracy: 0.78125\n",
      "Step: 2960 Loss: 0.757144 Accuracy: 0.8125\n",
      "Switched CIFAR set to 1\n",
      "Step: 2970 Loss: 0.800048 Accuracy: 0.796875\n",
      "Step: 2980 Loss: 0.83183 Accuracy: 0.765625\n",
      "Step: 2990 Loss: 0.991395 Accuracy: 0.71875\n",
      "Step: 3000 Loss: 0.743007 Accuracy: 0.828125\n",
      "Test set accuracy: 0.728\n",
      "Step: 3010 Loss: 0.970164 Accuracy: 0.734375\n",
      "Step: 3020 Loss: 0.697645 Accuracy: 0.84375\n",
      "Step: 3030 Loss: 0.807321 Accuracy: 0.765625\n",
      "Step: 3040 Loss: 0.783615 Accuracy: 0.8125\n",
      "Step: 3050 Loss: 0.761195 Accuracy: 0.796875\n",
      "Step: 3060 Loss: 0.834175 Accuracy: 0.765625\n",
      "Step: 3070 Loss: 0.79593 Accuracy: 0.8125\n",
      "Step: 3080 Loss: 0.852917 Accuracy: 0.78125\n",
      "Step: 3090 Loss: 0.757507 Accuracy: 0.828125\n",
      "Step: 3100 Loss: 0.898478 Accuracy: 0.765625\n",
      "Test set accuracy: 0.72\n",
      "Step: 3110 Loss: 0.83091 Accuracy: 0.75\n",
      "Switched CIFAR set to 2\n",
      "Step: 3120 Loss: 0.804697 Accuracy: 0.765625\n",
      "Step: 3130 Loss: 0.761518 Accuracy: 0.828125\n",
      "Step: 3140 Loss: 0.984067 Accuracy: 0.734375\n",
      "Step: 3150 Loss: 0.87854 Accuracy: 0.75\n",
      "Step: 3160 Loss: 0.731936 Accuracy: 0.796875\n",
      "Step: 3170 Loss: 0.932864 Accuracy: 0.71875\n",
      "Step: 3180 Loss: 0.871321 Accuracy: 0.75\n",
      "Step: 3190 Loss: 0.854096 Accuracy: 0.75\n",
      "Step: 3200 Loss: 0.792292 Accuracy: 0.828125\n",
      "Test set accuracy: 0.712\n",
      "Step: 3210 Loss: 0.786005 Accuracy: 0.8125\n",
      "Step: 3220 Loss: 0.661638 Accuracy: 0.859375\n",
      "Step: 3230 Loss: 0.601287 Accuracy: 0.875\n",
      "Step: 3240 Loss: 0.680492 Accuracy: 0.828125\n",
      "Step: 3250 Loss: 0.683048 Accuracy: 0.859375\n",
      "Step: 3260 Loss: 0.715356 Accuracy: 0.84375\n",
      "Step: 3270 Loss: 0.69214 Accuracy: 0.875\n",
      "Switched CIFAR set to 3\n",
      "Step: 3280 Loss: 1.00197 Accuracy: 0.78125\n",
      "Step: 3290 Loss: 0.785882 Accuracy: 0.796875\n",
      "Step: 3300 Loss: 0.768923 Accuracy: 0.8125\n",
      "Test set accuracy: 0.756\n",
      "Step: 3310 Loss: 0.718487 Accuracy: 0.8125\n",
      "Step: 3320 Loss: 0.702828 Accuracy: 0.8125\n",
      "Step: 3330 Loss: 0.666502 Accuracy: 0.859375\n",
      "Step: 3340 Loss: 0.942592 Accuracy: 0.75\n",
      "Step: 3350 Loss: 0.660728 Accuracy: 0.84375\n",
      "Step: 3360 Loss: 0.779213 Accuracy: 0.828125\n",
      "Step: 3370 Loss: 0.83558 Accuracy: 0.8125\n",
      "Step: 3380 Loss: 0.71852 Accuracy: 0.8125\n",
      "Step: 3390 Loss: 0.691263 Accuracy: 0.859375\n",
      "Step: 3400 Loss: 0.642181 Accuracy: 0.859375\n",
      "Test set accuracy: 0.78\n",
      "Step: 3410 Loss: 0.722249 Accuracy: 0.84375\n",
      "Step: 3420 Loss: 0.798818 Accuracy: 0.78125\n",
      "Step: 3430 Loss: 0.650799 Accuracy: 0.84375\n",
      "Switched CIFAR set to 4\n",
      "Step: 3440 Loss: 0.79231 Accuracy: 0.796875\n",
      "Step: 3450 Loss: 0.872037 Accuracy: 0.75\n",
      "Step: 3460 Loss: 1.00286 Accuracy: 0.71875\n",
      "Step: 3470 Loss: 0.867931 Accuracy: 0.75\n",
      "Step: 3480 Loss: 0.870014 Accuracy: 0.71875\n",
      "Step: 3490 Loss: 0.653076 Accuracy: 0.875\n",
      "Step: 3500 Loss: 0.885482 Accuracy: 0.78125\n",
      "Test set accuracy: 0.748\n",
      "Step: 3510 Loss: 0.60763 Accuracy: 0.890625\n",
      "Step: 3520 Loss: 0.82803 Accuracy: 0.78125\n",
      "Step: 3530 Loss: 0.951464 Accuracy: 0.75\n",
      "Step: 3540 Loss: 0.685493 Accuracy: 0.8125\n",
      "Step: 3550 Loss: 0.860613 Accuracy: 0.8125\n",
      "Step: 3560 Loss: 0.634371 Accuracy: 0.859375\n",
      "Step: 3570 Loss: 0.742345 Accuracy: 0.8125\n",
      "Step: 3580 Loss: 0.956353 Accuracy: 0.6875\n",
      "Switched CIFAR set to 5\n",
      "Step: 3590 Loss: 0.92096 Accuracy: 0.765625\n",
      "Step: 3600 Loss: 0.858479 Accuracy: 0.78125\n",
      "Test set accuracy: 0.766\n",
      "Step: 3610 Loss: 0.625523 Accuracy: 0.828125\n",
      "Step: 3620 Loss: 0.727972 Accuracy: 0.8125\n",
      "Step: 3630 Loss: 0.609086 Accuracy: 0.890625\n",
      "Step: 3640 Loss: 0.569573 Accuracy: 0.921875\n",
      "Step: 3650 Loss: 0.861142 Accuracy: 0.765625\n",
      "Step: 3660 Loss: 0.674751 Accuracy: 0.859375\n",
      "Step: 3670 Loss: 0.678256 Accuracy: 0.8125\n",
      "Step: 3680 Loss: 0.752368 Accuracy: 0.84375\n",
      "Step: 3690 Loss: 0.646419 Accuracy: 0.828125\n",
      "Step: 3700 Loss: 0.661113 Accuracy: 0.890625\n",
      "Test set accuracy: 0.742\n",
      "Step: 3710 Loss: 0.750337 Accuracy: 0.84375\n",
      "Step: 3720 Loss: 0.657263 Accuracy: 0.890625\n",
      "Step: 3730 Loss: 0.754511 Accuracy: 0.84375\n",
      "Step: 3740 Loss: 0.576733 Accuracy: 0.875\n",
      "Switched CIFAR set to 1\n",
      "Step: 3750 Loss: 0.891268 Accuracy: 0.765625\n",
      "Step: 3760 Loss: 0.756644 Accuracy: 0.78125\n",
      "Step: 3770 Loss: 0.928131 Accuracy: 0.703125\n",
      "Step: 3780 Loss: 0.670344 Accuracy: 0.84375\n",
      "Step: 3790 Loss: 0.723415 Accuracy: 0.8125\n",
      "Step: 3800 Loss: 0.77627 Accuracy: 0.796875\n",
      "Test set accuracy: 0.77\n",
      "Step: 3810 Loss: 0.752649 Accuracy: 0.796875\n",
      "Step: 3820 Loss: 0.579242 Accuracy: 0.875\n",
      "Step: 3830 Loss: 0.643179 Accuracy: 0.84375\n",
      "Step: 3840 Loss: 0.684965 Accuracy: 0.84375\n",
      "Step: 3850 Loss: 0.645989 Accuracy: 0.859375\n",
      "Step: 3860 Loss: 1.05631 Accuracy: 0.703125\n",
      "Step: 3870 Loss: 0.681147 Accuracy: 0.796875\n",
      "Step: 3880 Loss: 0.589528 Accuracy: 0.890625\n",
      "Step: 3890 Loss: 0.588873 Accuracy: 0.890625\n",
      "Switched CIFAR set to 2\n",
      "Step: 3900 Loss: 0.6914 Accuracy: 0.828125\n",
      "Test set accuracy: 0.754\n",
      "Step: 3910 Loss: 0.813582 Accuracy: 0.796875\n",
      "Step: 3920 Loss: 0.719163 Accuracy: 0.84375\n",
      "Step: 3930 Loss: 0.712757 Accuracy: 0.875\n",
      "Step: 3940 Loss: 0.690493 Accuracy: 0.828125\n",
      "Step: 3950 Loss: 0.778056 Accuracy: 0.8125\n",
      "Step: 3960 Loss: 0.783034 Accuracy: 0.796875\n",
      "Step: 3970 Loss: 0.66064 Accuracy: 0.875\n",
      "Step: 3980 Loss: 0.653491 Accuracy: 0.875\n",
      "Step: 3990 Loss: 0.657647 Accuracy: 0.859375\n",
      "Step: 4000 Loss: 0.790662 Accuracy: 0.796875\n",
      "Test set accuracy: 0.794\n",
      "Step: 4010 Loss: 0.694216 Accuracy: 0.828125\n",
      "Step: 4020 Loss: 0.78556 Accuracy: 0.8125\n",
      "Step: 4030 Loss: 0.868197 Accuracy: 0.796875\n",
      "Step: 4040 Loss: 0.656632 Accuracy: 0.828125\n",
      "Step: 4050 Loss: 0.683799 Accuracy: 0.8125\n",
      "Switched CIFAR set to 3\n",
      "Step: 4060 Loss: 0.606263 Accuracy: 0.890625\n",
      "Step: 4070 Loss: 0.843408 Accuracy: 0.765625\n",
      "Step: 4080 Loss: 0.827562 Accuracy: 0.78125\n",
      "Step: 4090 Loss: 0.907569 Accuracy: 0.8125\n",
      "Step: 4100 Loss: 0.821416 Accuracy: 0.765625\n",
      "Test set accuracy: 0.802\n",
      "Step: 4110 Loss: 0.809189 Accuracy: 0.78125\n",
      "Step: 4120 Loss: 0.718713 Accuracy: 0.8125\n",
      "Step: 4130 Loss: 0.674031 Accuracy: 0.84375\n",
      "Step: 4140 Loss: 0.740194 Accuracy: 0.796875\n",
      "Step: 4150 Loss: 0.728832 Accuracy: 0.84375\n",
      "Step: 4160 Loss: 0.80036 Accuracy: 0.8125\n",
      "Step: 4170 Loss: 0.816865 Accuracy: 0.828125\n",
      "Step: 4180 Loss: 0.948828 Accuracy: 0.734375\n",
      "Step: 4190 Loss: 0.59668 Accuracy: 0.84375\n",
      "Step: 4200 Loss: 0.586673 Accuracy: 0.90625\n",
      "Test set accuracy: 0.774\n",
      "Step: 4210 Loss: 0.404608 Accuracy: 0.96875\n",
      "Switched CIFAR set to 4\n",
      "Step: 4220 Loss: 0.992959 Accuracy: 0.71875\n",
      "Step: 4230 Loss: 0.862512 Accuracy: 0.734375\n",
      "Step: 4240 Loss: 0.844427 Accuracy: 0.75\n",
      "Step: 4250 Loss: 0.640278 Accuracy: 0.859375\n",
      "Step: 4260 Loss: 0.710697 Accuracy: 0.796875\n",
      "Step: 4270 Loss: 0.693668 Accuracy: 0.8125\n",
      "Step: 4280 Loss: 0.714143 Accuracy: 0.859375\n",
      "Step: 4290 Loss: 0.688414 Accuracy: 0.84375\n",
      "Step: 4300 Loss: 0.605963 Accuracy: 0.875\n",
      "Test set accuracy: 0.786\n",
      "Step: 4310 Loss: 0.666959 Accuracy: 0.859375\n",
      "Step: 4320 Loss: 0.776184 Accuracy: 0.796875\n",
      "Step: 4330 Loss: 0.467317 Accuracy: 0.953125\n",
      "Step: 4340 Loss: 0.599904 Accuracy: 0.890625\n",
      "Step: 4350 Loss: 0.734538 Accuracy: 0.828125\n",
      "Step: 4360 Loss: 0.696487 Accuracy: 0.875\n",
      "Switched CIFAR set to 5\n",
      "Step: 4370 Loss: 0.993445 Accuracy: 0.75\n",
      "Step: 4380 Loss: 0.746717 Accuracy: 0.8125\n",
      "Step: 4390 Loss: 0.583772 Accuracy: 0.890625\n",
      "Step: 4400 Loss: 0.820929 Accuracy: 0.75\n",
      "Test set accuracy: 0.772\n",
      "Step: 4410 Loss: 0.779227 Accuracy: 0.796875\n",
      "Step: 4420 Loss: 0.676198 Accuracy: 0.875\n",
      "Step: 4430 Loss: 0.653656 Accuracy: 0.890625\n",
      "Step: 4440 Loss: 0.593456 Accuracy: 0.828125\n",
      "Step: 4450 Loss: 0.578296 Accuracy: 0.890625\n",
      "Step: 4460 Loss: 0.652952 Accuracy: 0.828125\n",
      "Step: 4470 Loss: 0.556628 Accuracy: 0.890625\n",
      "Step: 4480 Loss: 0.670988 Accuracy: 0.859375\n",
      "Step: 4490 Loss: 0.609493 Accuracy: 0.84375\n",
      "Step: 4500 Loss: 0.647303 Accuracy: 0.921875\n",
      "Test set accuracy: 0.792\n",
      "Step: 4510 Loss: 0.601746 Accuracy: 0.890625\n",
      "Step: 4520 Loss: 0.548796 Accuracy: 0.90625\n",
      "Switched CIFAR set to 1\n",
      "Step: 4530 Loss: 0.629175 Accuracy: 0.875\n",
      "Step: 4540 Loss: 0.774332 Accuracy: 0.8125\n",
      "Step: 4550 Loss: 0.700611 Accuracy: 0.828125\n",
      "Step: 4560 Loss: 0.725343 Accuracy: 0.78125\n",
      "Step: 4570 Loss: 0.599537 Accuracy: 0.890625\n",
      "Step: 4580 Loss: 0.683246 Accuracy: 0.859375\n",
      "Step: 4590 Loss: 0.560821 Accuracy: 0.875\n",
      "Step: 4600 Loss: 0.648441 Accuracy: 0.890625\n",
      "Test set accuracy: 0.802\n",
      "Step: 4610 Loss: 0.846381 Accuracy: 0.796875\n",
      "Step: 4620 Loss: 0.637095 Accuracy: 0.84375\n",
      "Step: 4630 Loss: 0.74396 Accuracy: 0.78125\n",
      "Step: 4640 Loss: 0.646921 Accuracy: 0.859375\n",
      "Step: 4650 Loss: 0.761517 Accuracy: 0.796875\n",
      "Step: 4660 Loss: 0.703654 Accuracy: 0.875\n",
      "Step: 4670 Loss: 0.67627 Accuracy: 0.828125\n",
      "Switched CIFAR set to 2\n",
      "Step: 4680 Loss: 0.778866 Accuracy: 0.765625\n",
      "Step: 4690 Loss: 0.722437 Accuracy: 0.8125\n",
      "Step: 4700 Loss: 0.841605 Accuracy: 0.78125\n",
      "Test set accuracy: 0.778\n",
      "Step: 4710 Loss: 0.695586 Accuracy: 0.828125\n",
      "Step: 4720 Loss: 0.497571 Accuracy: 0.9375\n",
      "Step: 4730 Loss: 0.683706 Accuracy: 0.859375\n",
      "Step: 4740 Loss: 0.844 Accuracy: 0.78125\n",
      "Step: 4750 Loss: 0.601881 Accuracy: 0.890625\n",
      "Step: 4760 Loss: 0.827066 Accuracy: 0.765625\n",
      "Step: 4770 Loss: 0.773733 Accuracy: 0.796875\n",
      "Step: 4780 Loss: 0.579022 Accuracy: 0.90625\n",
      "Step: 4790 Loss: 0.627871 Accuracy: 0.875\n",
      "Step: 4800 Loss: 0.924472 Accuracy: 0.71875\n",
      "Test set accuracy: 0.798\n",
      "Step: 4810 Loss: 0.649612 Accuracy: 0.875\n",
      "Step: 4820 Loss: 0.494242 Accuracy: 0.953125\n",
      "Step: 4830 Loss: 0.817928 Accuracy: 0.796875\n",
      "Switched CIFAR set to 3\n",
      "Step: 4840 Loss: 0.752057 Accuracy: 0.78125\n",
      "Step: 4850 Loss: 0.824583 Accuracy: 0.734375\n",
      "Step: 4860 Loss: 0.844066 Accuracy: 0.828125\n",
      "Step: 4870 Loss: 0.588946 Accuracy: 0.875\n",
      "Step: 4880 Loss: 0.68466 Accuracy: 0.859375\n",
      "Step: 4890 Loss: 0.685595 Accuracy: 0.859375\n",
      "Step: 4900 Loss: 0.716238 Accuracy: 0.828125\n",
      "Test set accuracy: 0.778\n",
      "Step: 4910 Loss: 0.566491 Accuracy: 0.921875\n",
      "Step: 4920 Loss: 0.85448 Accuracy: 0.765625\n",
      "Step: 4930 Loss: 0.560477 Accuracy: 0.890625\n",
      "Step: 4940 Loss: 0.788682 Accuracy: 0.796875\n",
      "Step: 4950 Loss: 0.535493 Accuracy: 0.890625\n",
      "Step: 4960 Loss: 0.53736 Accuracy: 0.890625\n",
      "Step: 4970 Loss: 0.566296 Accuracy: 0.875\n",
      "Step: 4980 Loss: 0.667766 Accuracy: 0.859375\n",
      "Step: 4990 Loss: 0.729521 Accuracy: 0.828125\n",
      "Switched CIFAR set to 4\n",
      "Step: 5000 Loss: 0.713429 Accuracy: 0.828125\n",
      "Test set accuracy: 0.768\n",
      "Step: 5010 Loss: 0.740267 Accuracy: 0.84375\n",
      "Step: 5020 Loss: 0.612914 Accuracy: 0.875\n",
      "Step: 5030 Loss: 0.720951 Accuracy: 0.890625\n",
      "Step: 5040 Loss: 0.626576 Accuracy: 0.90625\n",
      "Step: 5050 Loss: 0.689965 Accuracy: 0.828125\n",
      "Step: 5060 Loss: 0.751083 Accuracy: 0.8125\n",
      "Step: 5070 Loss: 0.645374 Accuracy: 0.859375\n",
      "Step: 5080 Loss: 0.642746 Accuracy: 0.859375\n",
      "Step: 5090 Loss: 0.697809 Accuracy: 0.859375\n",
      "Step: 5100 Loss: 0.590368 Accuracy: 0.875\n",
      "Test set accuracy: 0.81\n",
      "Step: 5110 Loss: 0.587144 Accuracy: 0.875\n",
      "Step: 5120 Loss: 0.67706 Accuracy: 0.828125\n",
      "Step: 5130 Loss: 0.535225 Accuracy: 0.90625\n",
      "Step: 5140 Loss: 0.560828 Accuracy: 0.90625\n",
      "Switched CIFAR set to 5\n",
      "Step: 5150 Loss: 0.681348 Accuracy: 0.765625\n",
      "Step: 5160 Loss: 0.646137 Accuracy: 0.84375\n",
      "Step: 5170 Loss: 0.805215 Accuracy: 0.765625\n",
      "Step: 5180 Loss: 0.593186 Accuracy: 0.890625\n",
      "Step: 5190 Loss: 0.575445 Accuracy: 0.890625\n",
      "Step: 5200 Loss: 0.473067 Accuracy: 0.9375\n",
      "Test set accuracy: 0.806\n",
      "Step: 5210 Loss: 0.50545 Accuracy: 0.9375\n",
      "Step: 5220 Loss: 0.771103 Accuracy: 0.828125\n",
      "Step: 5230 Loss: 0.653597 Accuracy: 0.859375\n",
      "Step: 5240 Loss: 0.775392 Accuracy: 0.78125\n",
      "Step: 5250 Loss: 0.554166 Accuracy: 0.921875\n",
      "Step: 5260 Loss: 0.649375 Accuracy: 0.828125\n",
      "Step: 5270 Loss: 0.667594 Accuracy: 0.875\n",
      "Step: 5280 Loss: 0.682928 Accuracy: 0.84375\n",
      "Step: 5290 Loss: 0.588653 Accuracy: 0.875\n",
      "Step: 5300 Loss: 0.650385 Accuracy: 0.875\n",
      "Test set accuracy: 0.782\n",
      "Switched CIFAR set to 1\n",
      "Step: 5310 Loss: 0.921281 Accuracy: 0.765625\n",
      "Step: 5320 Loss: 0.782284 Accuracy: 0.78125\n",
      "Step: 5330 Loss: 0.625164 Accuracy: 0.875\n",
      "Step: 5340 Loss: 0.77656 Accuracy: 0.796875\n",
      "Step: 5350 Loss: 0.842253 Accuracy: 0.796875\n",
      "Step: 5360 Loss: 0.78014 Accuracy: 0.8125\n",
      "Step: 5370 Loss: 0.784775 Accuracy: 0.84375\n",
      "Step: 5380 Loss: 0.608629 Accuracy: 0.875\n",
      "Step: 5390 Loss: 0.635782 Accuracy: 0.84375\n",
      "Step: 5400 Loss: 0.565196 Accuracy: 0.890625\n",
      "Test set accuracy: 0.796\n",
      "Step: 5410 Loss: 0.655622 Accuracy: 0.890625\n",
      "Step: 5420 Loss: 0.59934 Accuracy: 0.875\n",
      "Step: 5430 Loss: 0.60184 Accuracy: 0.875\n",
      "Step: 5440 Loss: 0.689537 Accuracy: 0.890625\n",
      "Step: 5450 Loss: 0.550841 Accuracy: 0.921875\n",
      "Switched CIFAR set to 2\n",
      "Step: 5460 Loss: 0.907525 Accuracy: 0.734375\n",
      "Step: 5470 Loss: 0.807697 Accuracy: 0.78125\n",
      "Step: 5480 Loss: 0.505779 Accuracy: 0.921875\n",
      "Step: 5490 Loss: 0.603364 Accuracy: 0.90625\n",
      "Step: 5500 Loss: 0.708623 Accuracy: 0.84375\n",
      "Test set accuracy: 0.822\n",
      "Step: 5510 Loss: 0.648606 Accuracy: 0.859375\n",
      "Step: 5520 Loss: 0.748001 Accuracy: 0.796875\n",
      "Step: 5530 Loss: 0.643694 Accuracy: 0.875\n",
      "Step: 5540 Loss: 0.524424 Accuracy: 0.9375\n",
      "Step: 5550 Loss: 0.937012 Accuracy: 0.703125\n",
      "Step: 5560 Loss: 0.641385 Accuracy: 0.84375\n",
      "Step: 5570 Loss: 0.578878 Accuracy: 0.9375\n",
      "Step: 5580 Loss: 0.445397 Accuracy: 0.96875\n",
      "Step: 5590 Loss: 0.64883 Accuracy: 0.84375\n",
      "Step: 5600 Loss: 0.564438 Accuracy: 0.890625\n",
      "Test set accuracy: 0.782\n",
      "Step: 5610 Loss: 0.511131 Accuracy: 0.921875\n",
      "Switched CIFAR set to 3\n",
      "Step: 5620 Loss: 0.905789 Accuracy: 0.765625\n",
      "Step: 5630 Loss: 0.761591 Accuracy: 0.8125\n",
      "Step: 5640 Loss: 0.731599 Accuracy: 0.84375\n",
      "Step: 5650 Loss: 0.747431 Accuracy: 0.8125\n",
      "Step: 5660 Loss: 0.764444 Accuracy: 0.8125\n",
      "Step: 5670 Loss: 0.929044 Accuracy: 0.734375\n",
      "Step: 5680 Loss: 0.618371 Accuracy: 0.875\n",
      "Step: 5690 Loss: 0.565079 Accuracy: 0.921875\n",
      "Step: 5700 Loss: 0.541511 Accuracy: 0.90625\n",
      "Test set accuracy: 0.786\n",
      "Step: 5710 Loss: 0.694612 Accuracy: 0.828125\n",
      "Step: 5720 Loss: 0.569315 Accuracy: 0.90625\n",
      "Step: 5730 Loss: 0.699202 Accuracy: 0.84375\n",
      "Step: 5740 Loss: 0.626752 Accuracy: 0.875\n",
      "Step: 5750 Loss: 0.746228 Accuracy: 0.78125\n",
      "Step: 5760 Loss: 0.457749 Accuracy: 0.9375\n",
      "Step: 5770 Loss: 0.675097 Accuracy: 0.84375\n",
      "Switched CIFAR set to 4\n",
      "Step: 5780 Loss: 0.819373 Accuracy: 0.765625\n",
      "Step: 5790 Loss: 0.713005 Accuracy: 0.84375\n",
      "Step: 5800 Loss: 0.580778 Accuracy: 0.9375\n",
      "Test set accuracy: 0.81\n",
      "Step: 5810 Loss: 0.76092 Accuracy: 0.8125\n",
      "Step: 5820 Loss: 0.589916 Accuracy: 0.859375\n",
      "Step: 5830 Loss: 0.559819 Accuracy: 0.890625\n",
      "Step: 5840 Loss: 0.587843 Accuracy: 0.875\n",
      "Step: 5850 Loss: 0.559938 Accuracy: 0.875\n",
      "Step: 5860 Loss: 0.548009 Accuracy: 0.890625\n",
      "Step: 5870 Loss: 0.538361 Accuracy: 0.90625\n",
      "Step: 5880 Loss: 0.6442 Accuracy: 0.890625\n",
      "Step: 5890 Loss: 0.622294 Accuracy: 0.90625\n",
      "Step: 5900 Loss: 0.670516 Accuracy: 0.84375\n",
      "Test set accuracy: 0.802\n",
      "Step: 5910 Loss: 0.481625 Accuracy: 0.9375\n",
      "Step: 5920 Loss: 0.609201 Accuracy: 0.890625\n",
      "Switched CIFAR set to 5\n",
      "Step: 5930 Loss: 0.786235 Accuracy: 0.8125\n",
      "Step: 5940 Loss: 0.684021 Accuracy: 0.8125\n",
      "Step: 5950 Loss: 0.7628 Accuracy: 0.84375\n",
      "Step: 5960 Loss: 0.644814 Accuracy: 0.84375\n",
      "Step: 5970 Loss: 0.584316 Accuracy: 0.890625\n",
      "Step: 5980 Loss: 0.704856 Accuracy: 0.796875\n",
      "Step: 5990 Loss: 0.777216 Accuracy: 0.765625\n",
      "Step: 6000 Loss: 0.557152 Accuracy: 0.921875\n",
      "Test set accuracy: 0.81\n",
      "Step: 6010 Loss: 0.51677 Accuracy: 0.921875\n",
      "Step: 6020 Loss: 0.536227 Accuracy: 0.921875\n",
      "Step: 6030 Loss: 0.559817 Accuracy: 0.921875\n",
      "Step: 6040 Loss: 0.583359 Accuracy: 0.921875\n",
      "Step: 6050 Loss: 0.549637 Accuracy: 0.921875\n",
      "Step: 6060 Loss: 0.486985 Accuracy: 0.9375\n",
      "Step: 6070 Loss: 0.595302 Accuracy: 0.875\n",
      "Step: 6080 Loss: 0.633725 Accuracy: 0.890625\n",
      "Switched CIFAR set to 1\n",
      "Step: 6090 Loss: 0.758275 Accuracy: 0.859375\n",
      "Step: 6100 Loss: 0.549955 Accuracy: 0.90625\n",
      "Test set accuracy: 0.798\n",
      "Step: 6110 Loss: 0.817236 Accuracy: 0.78125\n",
      "Step: 6120 Loss: 0.618263 Accuracy: 0.875\n",
      "Step: 6130 Loss: 0.526484 Accuracy: 0.90625\n",
      "Step: 6140 Loss: 0.518543 Accuracy: 0.921875\n",
      "Step: 6150 Loss: 0.803653 Accuracy: 0.796875\n",
      "Step: 6160 Loss: 0.610473 Accuracy: 0.90625\n",
      "Step: 6170 Loss: 0.869368 Accuracy: 0.796875\n",
      "Step: 6180 Loss: 0.445709 Accuracy: 0.96875\n",
      "Step: 6190 Loss: 0.468987 Accuracy: 0.9375\n",
      "Step: 6200 Loss: 0.506542 Accuracy: 0.921875\n",
      "Test set accuracy: 0.816\n",
      "Step: 6210 Loss: 0.45513 Accuracy: 0.953125\n",
      "Step: 6220 Loss: 0.579814 Accuracy: 0.90625\n",
      "Step: 6230 Loss: 0.542385 Accuracy: 0.90625\n",
      "Switched CIFAR set to 2\n",
      "Step: 6240 Loss: 0.838366 Accuracy: 0.796875\n",
      "Step: 6250 Loss: 0.810799 Accuracy: 0.828125\n",
      "Step: 6260 Loss: 0.725669 Accuracy: 0.8125\n",
      "Step: 6270 Loss: 0.772339 Accuracy: 0.8125\n",
      "Step: 6280 Loss: 0.616692 Accuracy: 0.875\n",
      "Step: 6290 Loss: 0.611524 Accuracy: 0.875\n",
      "Step: 6300 Loss: 0.564296 Accuracy: 0.875\n",
      "Test set accuracy: 0.794\n",
      "Step: 6310 Loss: 0.439656 Accuracy: 0.953125\n",
      "Step: 6320 Loss: 0.700331 Accuracy: 0.84375\n",
      "Step: 6330 Loss: 0.531101 Accuracy: 0.9375\n",
      "Step: 6340 Loss: 0.676519 Accuracy: 0.828125\n",
      "Step: 6350 Loss: 0.619085 Accuracy: 0.875\n",
      "Step: 6360 Loss: 0.420712 Accuracy: 0.96875\n",
      "Step: 6370 Loss: 0.564173 Accuracy: 0.90625\n",
      "Step: 6380 Loss: 0.69116 Accuracy: 0.875\n",
      "Step: 6390 Loss: 0.759982 Accuracy: 0.84375\n",
      "Switched CIFAR set to 3\n",
      "Step: 6400 Loss: 0.599173 Accuracy: 0.875\n",
      "Test set accuracy: 0.802\n",
      "Step: 6410 Loss: 0.715658 Accuracy: 0.828125\n",
      "Step: 6420 Loss: 0.768088 Accuracy: 0.84375\n",
      "Step: 6430 Loss: 0.689007 Accuracy: 0.84375\n",
      "Step: 6440 Loss: 0.761429 Accuracy: 0.8125\n",
      "Step: 6450 Loss: 0.719175 Accuracy: 0.828125\n",
      "Step: 6460 Loss: 0.744168 Accuracy: 0.8125\n",
      "Step: 6470 Loss: 0.630908 Accuracy: 0.859375\n",
      "Step: 6480 Loss: 0.599528 Accuracy: 0.84375\n",
      "Step: 6490 Loss: 0.662461 Accuracy: 0.890625\n",
      "Step: 6500 Loss: 0.565156 Accuracy: 0.90625\n",
      "Test set accuracy: 0.78\n",
      "Step: 6510 Loss: 0.500634 Accuracy: 0.9375\n",
      "Step: 6520 Loss: 0.567322 Accuracy: 0.90625\n",
      "Step: 6530 Loss: 0.529456 Accuracy: 0.90625\n",
      "Step: 6540 Loss: 0.480881 Accuracy: 0.9375\n",
      "Step: 6550 Loss: 0.586701 Accuracy: 0.890625\n",
      "Switched CIFAR set to 4\n",
      "Step: 6560 Loss: 0.757419 Accuracy: 0.859375\n",
      "Step: 6570 Loss: 0.734842 Accuracy: 0.8125\n",
      "Step: 6580 Loss: 0.699146 Accuracy: 0.796875\n",
      "Step: 6590 Loss: 0.479315 Accuracy: 0.90625\n",
      "Step: 6600 Loss: 0.538769 Accuracy: 0.890625\n",
      "Test set accuracy: 0.804\n",
      "Step: 6610 Loss: 0.902631 Accuracy: 0.734375\n",
      "Step: 6620 Loss: 0.583716 Accuracy: 0.875\n",
      "Step: 6630 Loss: 0.672756 Accuracy: 0.796875\n",
      "Step: 6640 Loss: 0.535388 Accuracy: 0.9375\n",
      "Step: 6650 Loss: 0.80048 Accuracy: 0.765625\n",
      "Step: 6660 Loss: 0.658943 Accuracy: 0.890625\n",
      "Step: 6670 Loss: 0.714095 Accuracy: 0.8125\n",
      "Step: 6680 Loss: 0.512633 Accuracy: 0.90625\n",
      "Step: 6690 Loss: 0.434019 Accuracy: 0.96875\n",
      "Step: 6700 Loss: 0.558644 Accuracy: 0.90625\n",
      "Test set accuracy: 0.796\n",
      "Switched CIFAR set to 5\n",
      "Step: 6710 Loss: 0.616322 Accuracy: 0.90625\n",
      "Step: 6720 Loss: 0.749883 Accuracy: 0.78125\n",
      "Step: 6730 Loss: 0.780165 Accuracy: 0.8125\n",
      "Step: 6740 Loss: 0.592001 Accuracy: 0.90625\n",
      "Step: 6750 Loss: 0.6798 Accuracy: 0.828125\n",
      "Step: 6760 Loss: 0.634013 Accuracy: 0.875\n",
      "Step: 6770 Loss: 0.654956 Accuracy: 0.875\n",
      "Step: 6780 Loss: 0.633685 Accuracy: 0.859375\n",
      "Step: 6790 Loss: 0.684134 Accuracy: 0.828125\n",
      "Step: 6800 Loss: 0.463139 Accuracy: 0.96875\n",
      "Test set accuracy: 0.82\n",
      "Step: 6810 Loss: 0.604222 Accuracy: 0.875\n",
      "Step: 6820 Loss: 0.533635 Accuracy: 0.921875\n",
      "Step: 6830 Loss: 0.530053 Accuracy: 0.90625\n",
      "Step: 6840 Loss: 0.402287 Accuracy: 0.96875\n",
      "Step: 6850 Loss: 0.643004 Accuracy: 0.875\n",
      "Step: 6860 Loss: 0.581972 Accuracy: 0.90625\n",
      "Switched CIFAR set to 1\n",
      "Step: 6870 Loss: 0.686618 Accuracy: 0.859375\n",
      "Step: 6880 Loss: 0.574195 Accuracy: 0.875\n",
      "Step: 6890 Loss: 0.753928 Accuracy: 0.8125\n",
      "Step: 6900 Loss: 0.448993 Accuracy: 0.921875\n",
      "Test set accuracy: 0.804\n",
      "Step: 6910 Loss: 0.539211 Accuracy: 0.9375\n",
      "Step: 6920 Loss: 0.492879 Accuracy: 0.953125\n",
      "Step: 6930 Loss: 0.747592 Accuracy: 0.828125\n",
      "Step: 6940 Loss: 0.562437 Accuracy: 0.890625\n",
      "Step: 6950 Loss: 0.575784 Accuracy: 0.890625\n",
      "Step: 6960 Loss: 0.446779 Accuracy: 0.96875\n",
      "Step: 6970 Loss: 0.463483 Accuracy: 0.921875\n",
      "Step: 6980 Loss: 0.691846 Accuracy: 0.84375\n",
      "Step: 6990 Loss: 0.427685 Accuracy: 0.90625\n",
      "Step: 7000 Loss: 0.535304 Accuracy: 0.890625\n",
      "Test set accuracy: 0.812\n",
      "Step: 7010 Loss: 0.462823 Accuracy: 0.953125\n",
      "Switched CIFAR set to 2\n",
      "Step: 7020 Loss: 0.883021 Accuracy: 0.75\n",
      "Step: 7030 Loss: 0.473341 Accuracy: 0.921875\n",
      "Step: 7040 Loss: 0.546749 Accuracy: 0.875\n",
      "Step: 7050 Loss: 0.61812 Accuracy: 0.875\n",
      "Step: 7060 Loss: 0.580463 Accuracy: 0.875\n",
      "Step: 7070 Loss: 0.60537 Accuracy: 0.90625\n",
      "Step: 7080 Loss: 0.670314 Accuracy: 0.875\n",
      "Step: 7090 Loss: 0.574172 Accuracy: 0.890625\n",
      "Step: 7100 Loss: 0.602735 Accuracy: 0.890625\n",
      "Test set accuracy: 0.832\n",
      "Step: 7110 Loss: 0.578027 Accuracy: 0.90625\n",
      "Step: 7120 Loss: 0.472749 Accuracy: 0.9375\n",
      "Step: 7130 Loss: 0.621909 Accuracy: 0.859375\n",
      "Step: 7140 Loss: 0.704183 Accuracy: 0.796875\n",
      "Step: 7150 Loss: 0.554208 Accuracy: 0.890625\n",
      "Step: 7160 Loss: 0.581572 Accuracy: 0.875\n",
      "Step: 7170 Loss: 0.48353 Accuracy: 0.9375\n",
      "Switched CIFAR set to 3\n",
      "Step: 7180 Loss: 0.689932 Accuracy: 0.859375\n",
      "Step: 7190 Loss: 0.579115 Accuracy: 0.890625\n",
      "Step: 7200 Loss: 0.598268 Accuracy: 0.890625\n",
      "Test set accuracy: 0.766\n",
      "Step: 7210 Loss: 0.553873 Accuracy: 0.90625\n",
      "Step: 7220 Loss: 0.515799 Accuracy: 0.921875\n",
      "Step: 7230 Loss: 0.494795 Accuracy: 0.921875\n",
      "Step: 7240 Loss: 0.413551 Accuracy: 0.921875\n",
      "Step: 7250 Loss: 0.504087 Accuracy: 0.9375\n",
      "Step: 7260 Loss: 0.518306 Accuracy: 0.921875\n",
      "Step: 7270 Loss: 0.653241 Accuracy: 0.859375\n",
      "Step: 7280 Loss: 0.631349 Accuracy: 0.875\n",
      "Step: 7290 Loss: 0.454054 Accuracy: 0.953125\n",
      "Step: 7300 Loss: 0.587371 Accuracy: 0.875\n",
      "Test set accuracy: 0.79\n",
      "Step: 7310 Loss: 0.495552 Accuracy: 0.9375\n",
      "Step: 7320 Loss: 0.587544 Accuracy: 0.875\n",
      "Step: 7330 Loss: 0.514737 Accuracy: 0.921875\n",
      "Switched CIFAR set to 4\n",
      "Step: 7340 Loss: 0.697577 Accuracy: 0.8125\n",
      "Step: 7350 Loss: 0.730219 Accuracy: 0.84375\n",
      "Step: 7360 Loss: 0.513879 Accuracy: 0.90625\n",
      "Step: 7370 Loss: 0.672046 Accuracy: 0.875\n",
      "Step: 7380 Loss: 0.64944 Accuracy: 0.859375\n",
      "Step: 7390 Loss: 0.709863 Accuracy: 0.84375\n",
      "Step: 7400 Loss: 0.626224 Accuracy: 0.890625\n",
      "Test set accuracy: 0.82\n",
      "Step: 7410 Loss: 0.495489 Accuracy: 0.9375\n",
      "Step: 7420 Loss: 0.570888 Accuracy: 0.84375\n",
      "Step: 7430 Loss: 0.792839 Accuracy: 0.84375\n",
      "Step: 7440 Loss: 0.724131 Accuracy: 0.84375\n",
      "Step: 7450 Loss: 0.55741 Accuracy: 0.890625\n",
      "Step: 7460 Loss: 0.655382 Accuracy: 0.859375\n",
      "Step: 7470 Loss: 0.58499 Accuracy: 0.90625\n",
      "Step: 7480 Loss: 0.471328 Accuracy: 0.921875\n",
      "Switched CIFAR set to 5\n",
      "Step: 7490 Loss: 0.695764 Accuracy: 0.828125\n",
      "Step: 7500 Loss: 0.595749 Accuracy: 0.875\n",
      "Test set accuracy: 0.814\n",
      "Step: 7510 Loss: 0.601695 Accuracy: 0.890625\n",
      "Step: 7520 Loss: 0.502895 Accuracy: 0.9375\n",
      "Step: 7530 Loss: 0.546506 Accuracy: 0.90625\n",
      "Step: 7540 Loss: 0.840148 Accuracy: 0.765625\n",
      "Step: 7550 Loss: 0.58501 Accuracy: 0.875\n",
      "Step: 7560 Loss: 0.53461 Accuracy: 0.90625\n",
      "Step: 7570 Loss: 0.484389 Accuracy: 0.921875\n",
      "Step: 7580 Loss: 0.431963 Accuracy: 0.953125\n",
      "Step: 7590 Loss: 0.40168 Accuracy: 0.96875\n",
      "Step: 7600 Loss: 0.550782 Accuracy: 0.90625\n",
      "Test set accuracy: 0.838\n",
      "Step: 7610 Loss: 0.626105 Accuracy: 0.875\n",
      "Step: 7620 Loss: 0.392187 Accuracy: 0.984375\n",
      "Step: 7630 Loss: 0.538518 Accuracy: 0.921875\n",
      "Step: 7640 Loss: 0.513321 Accuracy: 0.921875\n",
      "Switched CIFAR set to 1\n",
      "Step: 7650 Loss: 0.680601 Accuracy: 0.796875\n",
      "Step: 7660 Loss: 0.591843 Accuracy: 0.90625\n",
      "Step: 7670 Loss: 0.63367 Accuracy: 0.90625\n",
      "Step: 7680 Loss: 0.569878 Accuracy: 0.859375\n",
      "Step: 7690 Loss: 0.479396 Accuracy: 0.90625\n",
      "Step: 7700 Loss: 0.505614 Accuracy: 0.953125\n",
      "Test set accuracy: 0.8\n",
      "Step: 7710 Loss: 0.496658 Accuracy: 0.921875\n",
      "Step: 7720 Loss: 0.451538 Accuracy: 0.953125\n",
      "Step: 7730 Loss: 0.447932 Accuracy: 1.0\n",
      "Step: 7740 Loss: 0.626768 Accuracy: 0.890625\n",
      "Step: 7750 Loss: 0.551019 Accuracy: 0.921875\n",
      "Step: 7760 Loss: 0.505218 Accuracy: 0.921875\n",
      "Step: 7770 Loss: 0.481164 Accuracy: 0.953125\n",
      "Step: 7780 Loss: 0.685285 Accuracy: 0.828125\n",
      "Step: 7790 Loss: 0.528068 Accuracy: 0.9375\n",
      "Switched CIFAR set to 2\n",
      "Step: 7800 Loss: 0.639408 Accuracy: 0.859375\n",
      "Test set accuracy: 0.8\n",
      "Step: 7810 Loss: 0.539768 Accuracy: 0.890625\n",
      "Step: 7820 Loss: 0.594995 Accuracy: 0.875\n",
      "Step: 7830 Loss: 0.658738 Accuracy: 0.890625\n",
      "Step: 7840 Loss: 0.529626 Accuracy: 0.921875\n",
      "Step: 7850 Loss: 0.588579 Accuracy: 0.90625\n",
      "Step: 7860 Loss: 0.773373 Accuracy: 0.78125\n",
      "Step: 7870 Loss: 0.525268 Accuracy: 0.921875\n",
      "Step: 7880 Loss: 0.567534 Accuracy: 0.890625\n",
      "Step: 7890 Loss: 0.596755 Accuracy: 0.875\n",
      "Step: 7900 Loss: 0.490147 Accuracy: 0.9375\n",
      "Test set accuracy: 0.826\n",
      "Step: 7910 Loss: 0.511907 Accuracy: 0.921875\n",
      "Step: 7920 Loss: 0.611924 Accuracy: 0.875\n",
      "Step: 7930 Loss: 0.545726 Accuracy: 0.90625\n",
      "Step: 7940 Loss: 0.496893 Accuracy: 0.9375\n",
      "Step: 7950 Loss: 0.631583 Accuracy: 0.8125\n",
      "Switched CIFAR set to 3\n",
      "Step: 7960 Loss: 0.824579 Accuracy: 0.8125\n",
      "Step: 7970 Loss: 0.547774 Accuracy: 0.90625\n",
      "Step: 7980 Loss: 0.501801 Accuracy: 0.953125\n",
      "Step: 7990 Loss: 0.389697 Accuracy: 0.953125\n",
      "Step: 8000 Loss: 0.597096 Accuracy: 0.890625\n",
      "Test set accuracy: 0.792\n",
      "Step: 8010 Loss: 0.514792 Accuracy: 0.921875\n",
      "Step: 8020 Loss: 0.463552 Accuracy: 0.953125\n",
      "Step: 8030 Loss: 0.558373 Accuracy: 0.9375\n",
      "Step: 8040 Loss: 0.678575 Accuracy: 0.828125\n",
      "Step: 8050 Loss: 0.487671 Accuracy: 0.9375\n",
      "Step: 8060 Loss: 0.434987 Accuracy: 0.953125\n",
      "Step: 8070 Loss: 0.545007 Accuracy: 0.875\n",
      "Step: 8080 Loss: 0.485582 Accuracy: 0.9375\n",
      "Step: 8090 Loss: 0.55933 Accuracy: 0.921875\n",
      "Step: 8100 Loss: 0.580536 Accuracy: 0.890625\n",
      "Test set accuracy: 0.808\n",
      "Step: 8110 Loss: 0.520908 Accuracy: 0.921875\n",
      "Switched CIFAR set to 4\n",
      "Step: 8120 Loss: 0.497783 Accuracy: 0.890625\n",
      "Step: 8130 Loss: 0.559018 Accuracy: 0.890625\n",
      "Step: 8140 Loss: 0.723559 Accuracy: 0.84375\n",
      "Step: 8150 Loss: 0.643204 Accuracy: 0.859375\n",
      "Step: 8160 Loss: 0.581577 Accuracy: 0.9375\n",
      "Step: 8170 Loss: 0.387376 Accuracy: 0.984375\n",
      "Step: 8180 Loss: 0.572146 Accuracy: 0.890625\n",
      "Step: 8190 Loss: 0.526881 Accuracy: 0.921875\n",
      "Step: 8200 Loss: 0.642066 Accuracy: 0.828125\n",
      "Test set accuracy: 0.81\n",
      "Step: 8210 Loss: 0.611729 Accuracy: 0.875\n",
      "Step: 8220 Loss: 0.413631 Accuracy: 0.953125\n",
      "Step: 8230 Loss: 0.466733 Accuracy: 0.890625\n",
      "Step: 8240 Loss: 0.465856 Accuracy: 0.9375\n",
      "Step: 8250 Loss: 0.554151 Accuracy: 0.890625\n",
      "Step: 8260 Loss: 0.487053 Accuracy: 0.90625\n",
      "Switched CIFAR set to 5\n",
      "Step: 8270 Loss: 0.516674 Accuracy: 0.90625\n",
      "Step: 8280 Loss: 0.599179 Accuracy: 0.875\n",
      "Step: 8290 Loss: 0.73539 Accuracy: 0.828125\n",
      "Step: 8300 Loss: 0.664107 Accuracy: 0.84375\n",
      "Test set accuracy: 0.832\n",
      "Step: 8310 Loss: 0.678688 Accuracy: 0.890625\n",
      "Step: 8320 Loss: 0.5252 Accuracy: 0.921875\n",
      "Step: 8330 Loss: 0.435614 Accuracy: 0.953125\n",
      "Step: 8340 Loss: 0.557721 Accuracy: 0.9375\n",
      "Step: 8350 Loss: 0.475936 Accuracy: 0.953125\n",
      "Step: 8360 Loss: 0.521733 Accuracy: 0.9375\n",
      "Step: 8370 Loss: 0.431554 Accuracy: 0.9375\n",
      "Step: 8380 Loss: 0.512472 Accuracy: 0.9375\n",
      "Step: 8390 Loss: 0.542258 Accuracy: 0.90625\n",
      "Step: 8400 Loss: 0.437302 Accuracy: 0.96875\n",
      "Test set accuracy: 0.828\n",
      "Step: 8410 Loss: 0.362974 Accuracy: 1.0\n",
      "Step: 8420 Loss: 0.433797 Accuracy: 0.9375\n",
      "Switched CIFAR set to 1\n",
      "Step: 8430 Loss: 0.544167 Accuracy: 0.9375\n",
      "Step: 8440 Loss: 0.59039 Accuracy: 0.90625\n",
      "Step: 8450 Loss: 0.519141 Accuracy: 0.921875\n",
      "Step: 8460 Loss: 0.617812 Accuracy: 0.84375\n",
      "Step: 8470 Loss: 0.558413 Accuracy: 0.890625\n",
      "Step: 8480 Loss: 0.529572 Accuracy: 0.875\n",
      "Step: 8490 Loss: 0.601792 Accuracy: 0.875\n",
      "Step: 8500 Loss: 0.59828 Accuracy: 0.890625\n",
      "Test set accuracy: 0.828\n",
      "Step: 8510 Loss: 0.518223 Accuracy: 0.921875\n",
      "Step: 8520 Loss: 0.716889 Accuracy: 0.84375\n",
      "Step: 8530 Loss: 0.43546 Accuracy: 0.96875\n",
      "Step: 8540 Loss: 0.391518 Accuracy: 0.984375\n",
      "Step: 8550 Loss: 0.460652 Accuracy: 0.921875\n",
      "Step: 8560 Loss: 0.518465 Accuracy: 0.890625\n",
      "Step: 8570 Loss: 0.511854 Accuracy: 0.921875\n",
      "Switched CIFAR set to 2\n",
      "Step: 8580 Loss: 0.705564 Accuracy: 0.828125\n",
      "Step: 8590 Loss: 0.528614 Accuracy: 0.890625\n",
      "Step: 8600 Loss: 0.706626 Accuracy: 0.859375\n",
      "Test set accuracy: 0.794\n",
      "Step: 8610 Loss: 0.508615 Accuracy: 0.90625\n",
      "Step: 8620 Loss: 0.646532 Accuracy: 0.890625\n",
      "Step: 8630 Loss: 0.575152 Accuracy: 0.890625\n",
      "Step: 8640 Loss: 0.535124 Accuracy: 0.921875\n",
      "Step: 8650 Loss: 0.565877 Accuracy: 0.921875\n",
      "Step: 8660 Loss: 0.451809 Accuracy: 0.9375\n",
      "Step: 8670 Loss: 0.639872 Accuracy: 0.890625\n",
      "Step: 8680 Loss: 0.43368 Accuracy: 0.953125\n",
      "Step: 8690 Loss: 0.529545 Accuracy: 0.890625\n",
      "Step: 8700 Loss: 0.483552 Accuracy: 0.9375\n",
      "Test set accuracy: 0.806\n",
      "Step: 8710 Loss: 0.522062 Accuracy: 0.90625\n",
      "Step: 8720 Loss: 0.469124 Accuracy: 0.921875\n",
      "Step: 8730 Loss: 0.584414 Accuracy: 0.890625\n",
      "Switched CIFAR set to 3\n",
      "Step: 8740 Loss: 0.518506 Accuracy: 0.890625\n",
      "Step: 8750 Loss: 0.541181 Accuracy: 0.90625\n",
      "Step: 8760 Loss: 0.597619 Accuracy: 0.875\n",
      "Step: 8770 Loss: 0.509584 Accuracy: 0.9375\n",
      "Step: 8780 Loss: 0.512622 Accuracy: 0.90625\n",
      "Step: 8790 Loss: 0.570393 Accuracy: 0.890625\n",
      "Step: 8800 Loss: 0.683224 Accuracy: 0.859375\n",
      "Test set accuracy: 0.798\n",
      "Step: 8810 Loss: 0.584435 Accuracy: 0.90625\n",
      "Step: 8820 Loss: 0.544785 Accuracy: 0.921875\n",
      "Step: 8830 Loss: 0.560401 Accuracy: 0.890625\n",
      "Step: 8840 Loss: 0.5933 Accuracy: 0.90625\n",
      "Step: 8850 Loss: 0.441955 Accuracy: 0.9375\n",
      "Step: 8860 Loss: 0.358687 Accuracy: 0.984375\n",
      "Step: 8870 Loss: 0.488333 Accuracy: 0.9375\n",
      "Step: 8880 Loss: 0.387831 Accuracy: 0.984375\n",
      "Step: 8890 Loss: 0.406215 Accuracy: 0.96875\n",
      "Switched CIFAR set to 4\n",
      "Step: 8900 Loss: 0.576278 Accuracy: 0.875\n",
      "Test set accuracy: 0.836\n",
      "Step: 8910 Loss: 0.547443 Accuracy: 0.890625\n",
      "Step: 8920 Loss: 0.581517 Accuracy: 0.890625\n",
      "Step: 8930 Loss: 0.650671 Accuracy: 0.875\n",
      "Step: 8940 Loss: 0.593463 Accuracy: 0.875\n",
      "Step: 8950 Loss: 0.590035 Accuracy: 0.84375\n",
      "Step: 8960 Loss: 0.469824 Accuracy: 0.921875\n",
      "Step: 8970 Loss: 0.379997 Accuracy: 0.96875\n",
      "Step: 8980 Loss: 0.523993 Accuracy: 0.921875\n",
      "Step: 8990 Loss: 0.573763 Accuracy: 0.890625\n",
      "Step: 9000 Loss: 0.38435 Accuracy: 0.984375\n",
      "Test set accuracy: 0.816\n",
      "Step: 9010 Loss: 0.690042 Accuracy: 0.84375\n",
      "Step: 9020 Loss: 0.602061 Accuracy: 0.875\n",
      "Step: 9030 Loss: 0.595914 Accuracy: 0.890625\n",
      "Step: 9040 Loss: 0.514586 Accuracy: 0.921875\n",
      "Switched CIFAR set to 5\n",
      "Step: 9050 Loss: 0.517238 Accuracy: 0.90625\n",
      "Step: 9060 Loss: 0.515587 Accuracy: 0.921875\n",
      "Step: 9070 Loss: 0.528735 Accuracy: 0.90625\n",
      "Step: 9080 Loss: 0.444424 Accuracy: 0.96875\n",
      "Step: 9090 Loss: 0.548058 Accuracy: 0.890625\n",
      "Step: 9100 Loss: 0.532351 Accuracy: 0.859375\n",
      "Test set accuracy: 0.83\n",
      "Step: 9110 Loss: 0.449069 Accuracy: 0.9375\n",
      "Step: 9120 Loss: 0.47383 Accuracy: 0.96875\n",
      "Step: 9130 Loss: 0.499208 Accuracy: 0.9375\n",
      "Step: 9140 Loss: 0.567721 Accuracy: 0.90625\n",
      "Step: 9150 Loss: 0.374167 Accuracy: 0.96875\n",
      "Step: 9160 Loss: 0.650891 Accuracy: 0.84375\n",
      "Step: 9170 Loss: 0.510997 Accuracy: 0.921875\n",
      "Step: 9180 Loss: 0.473175 Accuracy: 0.9375\n",
      "Step: 9190 Loss: 0.388592 Accuracy: 0.953125\n",
      "Step: 9200 Loss: 0.375716 Accuracy: 0.984375\n",
      "Test set accuracy: 0.816\n",
      "Switched CIFAR set to 1\n",
      "Step: 9210 Loss: 0.386722 Accuracy: 0.953125\n",
      "Step: 9220 Loss: 0.554344 Accuracy: 0.921875\n",
      "Step: 9230 Loss: 0.411519 Accuracy: 0.96875\n",
      "Step: 9240 Loss: 0.535586 Accuracy: 0.90625\n",
      "Step: 9250 Loss: 0.509791 Accuracy: 0.90625\n",
      "Step: 9260 Loss: 0.520176 Accuracy: 0.90625\n",
      "Step: 9270 Loss: 0.540916 Accuracy: 0.90625\n",
      "Step: 9280 Loss: 0.514561 Accuracy: 0.9375\n",
      "Step: 9290 Loss: 0.572938 Accuracy: 0.890625\n",
      "Step: 9300 Loss: 0.600376 Accuracy: 0.875\n",
      "Test set accuracy: 0.832\n",
      "Step: 9310 Loss: 0.583429 Accuracy: 0.859375\n",
      "Step: 9320 Loss: 0.492291 Accuracy: 0.921875\n",
      "Step: 9330 Loss: 0.447609 Accuracy: 0.921875\n",
      "Step: 9340 Loss: 0.468223 Accuracy: 0.953125\n",
      "Step: 9350 Loss: 0.622043 Accuracy: 0.84375\n",
      "Switched CIFAR set to 2\n",
      "Step: 9360 Loss: 0.675147 Accuracy: 0.84375\n",
      "Step: 9370 Loss: 0.483804 Accuracy: 0.9375\n",
      "Step: 9380 Loss: 0.54815 Accuracy: 0.921875\n",
      "Step: 9390 Loss: 0.614114 Accuracy: 0.859375\n",
      "Step: 9400 Loss: 0.611622 Accuracy: 0.875\n",
      "Test set accuracy: 0.844\n",
      "Step: 9410 Loss: 0.400387 Accuracy: 0.953125\n",
      "Step: 9420 Loss: 0.680506 Accuracy: 0.84375\n",
      "Step: 9430 Loss: 0.399276 Accuracy: 0.96875\n",
      "Step: 9440 Loss: 0.62059 Accuracy: 0.890625\n",
      "Step: 9450 Loss: 0.611299 Accuracy: 0.875\n",
      "Step: 9460 Loss: 0.51633 Accuracy: 0.90625\n",
      "Step: 9470 Loss: 0.640453 Accuracy: 0.875\n",
      "Step: 9480 Loss: 0.481322 Accuracy: 0.90625\n",
      "Step: 9490 Loss: 0.524006 Accuracy: 0.921875\n",
      "Step: 9500 Loss: 0.412801 Accuracy: 0.96875\n",
      "Test set accuracy: 0.82\n",
      "Step: 9510 Loss: 0.435271 Accuracy: 0.96875\n",
      "Switched CIFAR set to 3\n",
      "Step: 9520 Loss: 0.639842 Accuracy: 0.890625\n",
      "Step: 9530 Loss: 0.560539 Accuracy: 0.875\n",
      "Step: 9540 Loss: 0.603847 Accuracy: 0.890625\n",
      "Step: 9550 Loss: 0.479519 Accuracy: 0.921875\n",
      "Step: 9560 Loss: 0.596303 Accuracy: 0.890625\n",
      "Step: 9570 Loss: 0.621754 Accuracy: 0.875\n",
      "Step: 9580 Loss: 0.541993 Accuracy: 0.90625\n",
      "Step: 9590 Loss: 0.403116 Accuracy: 0.953125\n",
      "Step: 9600 Loss: 0.513613 Accuracy: 0.921875\n",
      "Test set accuracy: 0.804\n",
      "Step: 9610 Loss: 0.549839 Accuracy: 0.90625\n",
      "Step: 9620 Loss: 0.447974 Accuracy: 0.953125\n",
      "Step: 9630 Loss: 0.485723 Accuracy: 0.90625\n",
      "Step: 9640 Loss: 0.661475 Accuracy: 0.875\n",
      "Step: 9650 Loss: 0.437932 Accuracy: 0.96875\n",
      "Step: 9660 Loss: 0.41273 Accuracy: 0.96875\n",
      "Step: 9670 Loss: 0.330038 Accuracy: 1.0\n",
      "Switched CIFAR set to 4\n",
      "Step: 9680 Loss: 0.520856 Accuracy: 0.90625\n",
      "Step: 9690 Loss: 0.583194 Accuracy: 0.890625\n",
      "Step: 9700 Loss: 0.436717 Accuracy: 0.953125\n",
      "Test set accuracy: 0.824\n",
      "Step: 9710 Loss: 0.481913 Accuracy: 0.921875\n",
      "Step: 9720 Loss: 0.454797 Accuracy: 0.9375\n",
      "Step: 9730 Loss: 0.424801 Accuracy: 0.953125\n",
      "Step: 9740 Loss: 0.362841 Accuracy: 0.96875\n",
      "Step: 9750 Loss: 0.514473 Accuracy: 0.921875\n",
      "Step: 9760 Loss: 0.477754 Accuracy: 0.9375\n",
      "Step: 9770 Loss: 0.474527 Accuracy: 0.921875\n",
      "Step: 9780 Loss: 0.445771 Accuracy: 0.96875\n",
      "Step: 9790 Loss: 0.508799 Accuracy: 0.921875\n",
      "Step: 9800 Loss: 0.470225 Accuracy: 0.921875\n",
      "Test set accuracy: 0.804\n",
      "Step: 9810 Loss: 0.466287 Accuracy: 0.9375\n",
      "Step: 9820 Loss: 0.469367 Accuracy: 0.90625\n",
      "Switched CIFAR set to 5\n",
      "Step: 9830 Loss: 0.489696 Accuracy: 0.9375\n",
      "Step: 9840 Loss: 0.421161 Accuracy: 0.9375\n",
      "Step: 9850 Loss: 0.543976 Accuracy: 0.90625\n",
      "Step: 9860 Loss: 0.591016 Accuracy: 0.90625\n",
      "Step: 9870 Loss: 0.626236 Accuracy: 0.859375\n",
      "Step: 9880 Loss: 0.412379 Accuracy: 0.96875\n",
      "Step: 9890 Loss: 0.585163 Accuracy: 0.90625\n",
      "Step: 9900 Loss: 0.425779 Accuracy: 0.953125\n",
      "Test set accuracy: 0.794\n",
      "Step: 9910 Loss: 0.459288 Accuracy: 0.9375\n",
      "Step: 9920 Loss: 0.63314 Accuracy: 0.890625\n",
      "Step: 9930 Loss: 0.442534 Accuracy: 0.953125\n",
      "Step: 9940 Loss: 0.384726 Accuracy: 0.96875\n",
      "Step: 9950 Loss: 0.380333 Accuracy: 0.96875\n",
      "Step: 9960 Loss: 0.46295 Accuracy: 0.953125\n",
      "Step: 9970 Loss: 0.473766 Accuracy: 0.921875\n",
      "Step: 9980 Loss: 0.454118 Accuracy: 0.9375\n",
      "Switched CIFAR set to 1\n",
      "Step: 9990 Loss: 0.48234 Accuracy: 0.890625\n",
      "Step: 10000 Loss: 0.596131 Accuracy: 0.859375\n",
      "Test set accuracy: 0.794\n",
      "Step: 10010 Loss: 0.669009 Accuracy: 0.859375\n",
      "Step: 10020 Loss: 0.558568 Accuracy: 0.890625\n",
      "Step: 10030 Loss: 0.474442 Accuracy: 0.90625\n",
      "Step: 10040 Loss: 0.517489 Accuracy: 0.921875\n",
      "Step: 10050 Loss: 0.403123 Accuracy: 0.9375\n",
      "Step: 10060 Loss: 0.632638 Accuracy: 0.84375\n",
      "Step: 10070 Loss: 0.451595 Accuracy: 0.96875\n",
      "Step: 10080 Loss: 0.401229 Accuracy: 0.96875\n",
      "Step: 10090 Loss: 0.582968 Accuracy: 0.90625\n",
      "Step: 10100 Loss: 0.428338 Accuracy: 0.9375\n",
      "Test set accuracy: 0.816\n",
      "Step: 10110 Loss: 0.432032 Accuracy: 0.953125\n",
      "Step: 10120 Loss: 0.435979 Accuracy: 0.953125\n",
      "Step: 10130 Loss: 0.570011 Accuracy: 0.890625\n",
      "Switched CIFAR set to 2\n",
      "Step: 10140 Loss: 0.615115 Accuracy: 0.890625\n",
      "Step: 10150 Loss: 0.463731 Accuracy: 0.9375\n",
      "Step: 10160 Loss: 0.387361 Accuracy: 0.96875\n",
      "Step: 10170 Loss: 0.461203 Accuracy: 0.921875\n",
      "Step: 10180 Loss: 0.473851 Accuracy: 0.921875\n",
      "Step: 10190 Loss: 0.486955 Accuracy: 0.921875\n",
      "Step: 10200 Loss: 0.466424 Accuracy: 0.90625\n",
      "Test set accuracy: 0.838\n",
      "Step: 10210 Loss: 0.535854 Accuracy: 0.9375\n",
      "Step: 10220 Loss: 0.407385 Accuracy: 0.96875\n",
      "Step: 10230 Loss: 0.429574 Accuracy: 0.953125\n",
      "Step: 10240 Loss: 0.530445 Accuracy: 0.90625\n",
      "Step: 10250 Loss: 0.433888 Accuracy: 0.953125\n",
      "Step: 10260 Loss: 0.39394 Accuracy: 0.96875\n",
      "Step: 10270 Loss: 0.44001 Accuracy: 0.9375\n",
      "Step: 10280 Loss: 0.447527 Accuracy: 0.921875\n",
      "Step: 10290 Loss: 0.531439 Accuracy: 0.90625\n",
      "Switched CIFAR set to 3\n",
      "Step: 10300 Loss: 0.493759 Accuracy: 0.9375\n",
      "Test set accuracy: 0.828\n",
      "Step: 10310 Loss: 0.392393 Accuracy: 0.984375\n",
      "Step: 10320 Loss: 0.463035 Accuracy: 0.9375\n",
      "Step: 10330 Loss: 0.62241 Accuracy: 0.875\n",
      "Step: 10340 Loss: 0.476854 Accuracy: 0.9375\n",
      "Step: 10350 Loss: 0.515205 Accuracy: 0.90625\n",
      "Step: 10360 Loss: 0.517169 Accuracy: 0.890625\n",
      "Step: 10370 Loss: 0.57438 Accuracy: 0.90625\n",
      "Step: 10380 Loss: 0.566933 Accuracy: 0.90625\n",
      "Step: 10390 Loss: 0.628802 Accuracy: 0.890625\n",
      "Step: 10400 Loss: 0.453967 Accuracy: 0.9375\n",
      "Test set accuracy: 0.81\n",
      "Step: 10410 Loss: 0.488967 Accuracy: 0.90625\n",
      "Step: 10420 Loss: 0.528425 Accuracy: 0.90625\n",
      "Step: 10430 Loss: 0.440463 Accuracy: 0.953125\n",
      "Step: 10440 Loss: 0.549938 Accuracy: 0.90625\n",
      "Step: 10450 Loss: 0.372998 Accuracy: 0.953125\n",
      "Switched CIFAR set to 4\n",
      "Step: 10460 Loss: 0.473836 Accuracy: 0.921875\n",
      "Step: 10470 Loss: 0.465866 Accuracy: 0.9375\n",
      "Step: 10480 Loss: 0.732492 Accuracy: 0.828125\n",
      "Step: 10490 Loss: 0.613246 Accuracy: 0.875\n",
      "Step: 10500 Loss: 0.448756 Accuracy: 0.9375\n",
      "Test set accuracy: 0.82\n",
      "Step: 10510 Loss: 0.348542 Accuracy: 0.984375\n",
      "Step: 10520 Loss: 0.49882 Accuracy: 0.90625\n",
      "Step: 10530 Loss: 0.552307 Accuracy: 0.890625\n",
      "Step: 10540 Loss: 0.35561 Accuracy: 1.0\n",
      "Step: 10550 Loss: 0.532527 Accuracy: 0.921875\n",
      "Step: 10560 Loss: 0.470646 Accuracy: 0.9375\n",
      "Step: 10570 Loss: 0.475224 Accuracy: 0.90625\n",
      "Step: 10580 Loss: 0.476095 Accuracy: 0.953125\n",
      "Step: 10590 Loss: 0.669685 Accuracy: 0.84375\n",
      "Step: 10600 Loss: 0.468035 Accuracy: 0.90625\n",
      "Test set accuracy: 0.842\n",
      "Switched CIFAR set to 5\n",
      "Step: 10610 Loss: 0.574961 Accuracy: 0.890625\n",
      "Step: 10620 Loss: 0.527123 Accuracy: 0.890625\n",
      "Step: 10630 Loss: 0.560247 Accuracy: 0.921875\n",
      "Step: 10640 Loss: 0.452676 Accuracy: 0.9375\n",
      "Step: 10650 Loss: 0.423722 Accuracy: 0.9375\n",
      "Step: 10660 Loss: 0.459345 Accuracy: 0.96875\n",
      "Step: 10670 Loss: 0.443149 Accuracy: 0.953125\n",
      "Step: 10680 Loss: 0.426276 Accuracy: 0.9375\n",
      "Step: 10690 Loss: 0.448753 Accuracy: 0.96875\n",
      "Step: 10700 Loss: 0.422727 Accuracy: 0.96875\n",
      "Test set accuracy: 0.79\n",
      "Step: 10710 Loss: 0.356063 Accuracy: 0.984375\n",
      "Step: 10720 Loss: 0.402203 Accuracy: 0.953125\n",
      "Step: 10730 Loss: 0.518817 Accuracy: 0.890625\n",
      "Step: 10740 Loss: 0.399418 Accuracy: 0.953125\n",
      "Step: 10750 Loss: 0.491939 Accuracy: 0.90625\n",
      "Step: 10760 Loss: 0.714206 Accuracy: 0.84375\n",
      "Switched CIFAR set to 1\n",
      "Step: 10770 Loss: 0.480752 Accuracy: 0.921875\n",
      "Step: 10780 Loss: 0.592566 Accuracy: 0.875\n",
      "Step: 10790 Loss: 0.537007 Accuracy: 0.90625\n",
      "Step: 10800 Loss: 0.507142 Accuracy: 0.921875\n",
      "Test set accuracy: 0.814\n",
      "Step: 10810 Loss: 0.443398 Accuracy: 0.96875\n",
      "Step: 10820 Loss: 0.500207 Accuracy: 0.90625\n",
      "Step: 10830 Loss: 0.489119 Accuracy: 0.9375\n",
      "Step: 10840 Loss: 0.495638 Accuracy: 0.921875\n",
      "Step: 10850 Loss: 0.381459 Accuracy: 0.953125\n",
      "Step: 10860 Loss: 0.337464 Accuracy: 1.0\n",
      "Step: 10870 Loss: 0.469792 Accuracy: 0.953125\n",
      "Step: 10880 Loss: 0.503899 Accuracy: 0.9375\n",
      "Step: 10890 Loss: 0.425452 Accuracy: 0.96875\n",
      "Step: 10900 Loss: 0.375482 Accuracy: 0.953125\n",
      "Test set accuracy: 0.826\n",
      "Step: 10910 Loss: 0.388029 Accuracy: 0.9375\n",
      "Switched CIFAR set to 2\n",
      "Step: 10920 Loss: 0.555854 Accuracy: 0.90625\n",
      "Step: 10930 Loss: 0.518476 Accuracy: 0.921875\n",
      "Step: 10940 Loss: 0.404143 Accuracy: 0.96875\n",
      "Step: 10950 Loss: 0.520544 Accuracy: 0.90625\n",
      "Step: 10960 Loss: 0.465901 Accuracy: 0.921875\n",
      "Step: 10970 Loss: 0.379505 Accuracy: 0.984375\n",
      "Step: 10980 Loss: 0.571379 Accuracy: 0.875\n",
      "Step: 10990 Loss: 0.401724 Accuracy: 0.9375\n",
      "Step: 11000 Loss: 0.434556 Accuracy: 0.953125\n",
      "Test set accuracy: 0.802\n",
      "Step: 11010 Loss: 0.443496 Accuracy: 0.9375\n",
      "Step: 11020 Loss: 0.487305 Accuracy: 0.9375\n",
      "Step: 11030 Loss: 0.429722 Accuracy: 0.96875\n",
      "Step: 11040 Loss: 0.516104 Accuracy: 0.921875\n",
      "Step: 11050 Loss: 0.445064 Accuracy: 0.96875\n",
      "Step: 11060 Loss: 0.471542 Accuracy: 0.9375\n",
      "Step: 11070 Loss: 0.441279 Accuracy: 0.9375\n",
      "Switched CIFAR set to 3\n",
      "Step: 11080 Loss: 0.391857 Accuracy: 0.96875\n",
      "Step: 11090 Loss: 0.483183 Accuracy: 0.9375\n",
      "Step: 11100 Loss: 0.419325 Accuracy: 0.9375\n",
      "Test set accuracy: 0.834\n",
      "Step: 11110 Loss: 0.534932 Accuracy: 0.890625\n",
      "Step: 11120 Loss: 0.39277 Accuracy: 0.984375\n",
      "Step: 11130 Loss: 0.505752 Accuracy: 0.90625\n",
      "Step: 11140 Loss: 0.487745 Accuracy: 0.90625\n",
      "Step: 11150 Loss: 0.447436 Accuracy: 0.9375\n",
      "Step: 11160 Loss: 0.433603 Accuracy: 0.96875\n",
      "Step: 11170 Loss: 0.43122 Accuracy: 0.953125\n",
      "Step: 11180 Loss: 0.444886 Accuracy: 0.96875\n",
      "Step: 11190 Loss: 0.458972 Accuracy: 0.9375\n",
      "Step: 11200 Loss: 0.319439 Accuracy: 1.0\n",
      "Test set accuracy: 0.8\n",
      "Step: 11210 Loss: 0.441694 Accuracy: 0.9375\n",
      "Step: 11220 Loss: 0.355387 Accuracy: 0.984375\n",
      "Step: 11230 Loss: 0.403605 Accuracy: 0.953125\n",
      "Switched CIFAR set to 4\n",
      "Step: 11240 Loss: 0.504774 Accuracy: 0.921875\n",
      "Step: 11250 Loss: 0.501257 Accuracy: 0.90625\n",
      "Step: 11260 Loss: 0.497934 Accuracy: 0.96875\n",
      "Step: 11270 Loss: 0.571703 Accuracy: 0.890625\n",
      "Step: 11280 Loss: 0.44783 Accuracy: 0.9375\n",
      "Step: 11290 Loss: 0.586196 Accuracy: 0.90625\n",
      "Step: 11300 Loss: 0.439296 Accuracy: 0.984375\n",
      "Test set accuracy: 0.82\n",
      "Step: 11310 Loss: 0.416117 Accuracy: 0.96875\n",
      "Step: 11320 Loss: 0.468062 Accuracy: 0.921875\n",
      "Step: 11330 Loss: 0.387783 Accuracy: 0.984375\n",
      "Step: 11340 Loss: 0.443306 Accuracy: 0.953125\n",
      "Step: 11350 Loss: 0.357515 Accuracy: 0.96875\n",
      "Step: 11360 Loss: 0.486762 Accuracy: 0.921875\n",
      "Step: 11370 Loss: 0.487503 Accuracy: 0.890625\n",
      "Step: 11380 Loss: 0.474259 Accuracy: 0.9375\n",
      "Switched CIFAR set to 5\n",
      "Step: 11390 Loss: 0.49118 Accuracy: 0.9375\n",
      "Step: 11400 Loss: 0.412964 Accuracy: 0.96875\n",
      "Test set accuracy: 0.834\n",
      "Step: 11410 Loss: 0.466425 Accuracy: 0.921875\n",
      "Step: 11420 Loss: 0.465487 Accuracy: 0.921875\n",
      "Step: 11430 Loss: 0.447069 Accuracy: 0.953125\n",
      "Step: 11440 Loss: 0.592293 Accuracy: 0.90625\n",
      "Step: 11450 Loss: 0.365307 Accuracy: 0.984375\n",
      "Step: 11460 Loss: 0.471448 Accuracy: 0.921875\n",
      "Step: 11470 Loss: 0.510012 Accuracy: 0.921875\n",
      "Step: 11480 Loss: 0.464449 Accuracy: 0.9375\n",
      "Step: 11490 Loss: 0.426995 Accuracy: 0.96875\n",
      "Step: 11500 Loss: 0.727395 Accuracy: 0.875\n",
      "Test set accuracy: 0.824\n",
      "Step: 11510 Loss: 0.50003 Accuracy: 0.921875\n",
      "Step: 11520 Loss: 0.422108 Accuracy: 0.953125\n",
      "Step: 11530 Loss: 0.401193 Accuracy: 0.96875\n",
      "Step: 11540 Loss: 0.563142 Accuracy: 0.90625\n",
      "Switched CIFAR set to 1\n",
      "Step: 11550 Loss: 0.382721 Accuracy: 0.96875\n",
      "Step: 11560 Loss: 0.493013 Accuracy: 0.90625\n",
      "Step: 11570 Loss: 0.494961 Accuracy: 0.921875\n",
      "Step: 11580 Loss: 0.446534 Accuracy: 0.9375\n",
      "Step: 11590 Loss: 0.417199 Accuracy: 0.953125\n",
      "Step: 11600 Loss: 0.376595 Accuracy: 0.984375\n",
      "Test set accuracy: 0.834\n",
      "Step: 11610 Loss: 0.440939 Accuracy: 0.9375\n",
      "Step: 11620 Loss: 0.445949 Accuracy: 0.953125\n",
      "Step: 11630 Loss: 0.4612 Accuracy: 0.9375\n",
      "Step: 11640 Loss: 0.482172 Accuracy: 0.953125\n",
      "Step: 11650 Loss: 0.489236 Accuracy: 0.9375\n",
      "Step: 11660 Loss: 0.529811 Accuracy: 0.921875\n",
      "Step: 11670 Loss: 0.43274 Accuracy: 0.953125\n",
      "Step: 11680 Loss: 0.540039 Accuracy: 0.90625\n",
      "Step: 11690 Loss: 0.463153 Accuracy: 0.9375\n",
      "Switched CIFAR set to 2\n",
      "Step: 11700 Loss: 0.633223 Accuracy: 0.828125\n",
      "Test set accuracy: 0.828\n",
      "Step: 11710 Loss: 0.475935 Accuracy: 0.921875\n",
      "Step: 11720 Loss: 0.427377 Accuracy: 0.9375\n",
      "Step: 11730 Loss: 0.485492 Accuracy: 0.953125\n",
      "Step: 11740 Loss: 0.474012 Accuracy: 0.921875\n",
      "Step: 11750 Loss: 0.351944 Accuracy: 0.984375\n",
      "Step: 11760 Loss: 0.548635 Accuracy: 0.890625\n",
      "Step: 11770 Loss: 0.509506 Accuracy: 0.90625\n",
      "Step: 11780 Loss: 0.379076 Accuracy: 0.953125\n",
      "Step: 11790 Loss: 0.358742 Accuracy: 0.984375\n",
      "Step: 11800 Loss: 0.499883 Accuracy: 0.90625\n",
      "Test set accuracy: 0.844\n",
      "Step: 11810 Loss: 0.479139 Accuracy: 0.953125\n",
      "Step: 11820 Loss: 0.399069 Accuracy: 0.96875\n",
      "Step: 11830 Loss: 0.423139 Accuracy: 0.9375\n",
      "Step: 11840 Loss: 0.400493 Accuracy: 0.96875\n",
      "Step: 11850 Loss: 0.416156 Accuracy: 0.953125\n",
      "Switched CIFAR set to 3\n",
      "Step: 11860 Loss: 0.416107 Accuracy: 0.96875\n",
      "Step: 11870 Loss: 0.535653 Accuracy: 0.890625\n",
      "Step: 11880 Loss: 0.489888 Accuracy: 0.9375\n",
      "Step: 11890 Loss: 0.506281 Accuracy: 0.90625\n",
      "Step: 11900 Loss: 0.424363 Accuracy: 0.96875\n",
      "Test set accuracy: 0.846\n",
      "Step: 11910 Loss: 0.417431 Accuracy: 0.9375\n",
      "Step: 11920 Loss: 0.509925 Accuracy: 0.921875\n",
      "Step: 11930 Loss: 0.443904 Accuracy: 0.953125\n",
      "Step: 11940 Loss: 0.491385 Accuracy: 0.9375\n",
      "Step: 11950 Loss: 0.551838 Accuracy: 0.921875\n",
      "Step: 11960 Loss: 0.43055 Accuracy: 0.9375\n",
      "Step: 11970 Loss: 0.470493 Accuracy: 0.953125\n",
      "Step: 11980 Loss: 0.457959 Accuracy: 0.953125\n",
      "Step: 11990 Loss: 0.447334 Accuracy: 0.96875\n",
      "Step: 12000 Loss: 0.482277 Accuracy: 0.953125\n",
      "Test set accuracy: 0.832\n",
      "Step: 12010 Loss: 0.525058 Accuracy: 0.90625\n",
      "Switched CIFAR set to 4\n",
      "Step: 12020 Loss: 0.467256 Accuracy: 0.953125\n",
      "Step: 12030 Loss: 0.626627 Accuracy: 0.890625\n",
      "Step: 12040 Loss: 0.432421 Accuracy: 0.9375\n",
      "Step: 12050 Loss: 0.48148 Accuracy: 0.9375\n",
      "Step: 12060 Loss: 0.579431 Accuracy: 0.90625\n",
      "Step: 12070 Loss: 0.35419 Accuracy: 0.984375\n",
      "Step: 12080 Loss: 0.500421 Accuracy: 0.9375\n",
      "Step: 12090 Loss: 0.525642 Accuracy: 0.921875\n",
      "Step: 12100 Loss: 0.404649 Accuracy: 0.96875\n",
      "Test set accuracy: 0.836\n",
      "Step: 12110 Loss: 0.368408 Accuracy: 0.984375\n",
      "Step: 12120 Loss: 0.47621 Accuracy: 0.921875\n",
      "Step: 12130 Loss: 0.48561 Accuracy: 0.90625\n",
      "Step: 12140 Loss: 0.480874 Accuracy: 0.9375\n",
      "Step: 12150 Loss: 0.450502 Accuracy: 0.953125\n",
      "Step: 12160 Loss: 0.426688 Accuracy: 0.953125\n",
      "Switched CIFAR set to 5\n",
      "Step: 12170 Loss: 0.406986 Accuracy: 0.96875\n",
      "Step: 12180 Loss: 0.635176 Accuracy: 0.859375\n",
      "Step: 12190 Loss: 0.411427 Accuracy: 0.96875\n",
      "Step: 12200 Loss: 0.516553 Accuracy: 0.921875\n",
      "Test set accuracy: 0.836\n",
      "Step: 12210 Loss: 0.521675 Accuracy: 0.90625\n",
      "Step: 12220 Loss: 0.411421 Accuracy: 0.96875\n",
      "Step: 12230 Loss: 0.438212 Accuracy: 0.96875\n",
      "Step: 12240 Loss: 0.621306 Accuracy: 0.890625\n",
      "Step: 12250 Loss: 0.476668 Accuracy: 0.953125\n",
      "Step: 12260 Loss: 0.44286 Accuracy: 0.9375\n",
      "Step: 12270 Loss: 0.511369 Accuracy: 0.921875\n",
      "Step: 12280 Loss: 0.471961 Accuracy: 0.921875\n",
      "Step: 12290 Loss: 0.440063 Accuracy: 0.953125\n",
      "Step: 12300 Loss: 0.442937 Accuracy: 0.953125\n",
      "Test set accuracy: 0.842\n",
      "Step: 12310 Loss: 0.392814 Accuracy: 0.953125\n",
      "Step: 12320 Loss: 0.415613 Accuracy: 0.96875\n",
      "Switched CIFAR set to 1\n",
      "Step: 12330 Loss: 0.391107 Accuracy: 0.96875\n",
      "Step: 12340 Loss: 0.458347 Accuracy: 0.9375\n",
      "Step: 12350 Loss: 0.505042 Accuracy: 0.9375\n",
      "Step: 12360 Loss: 0.456832 Accuracy: 0.9375\n",
      "Step: 12370 Loss: 0.454338 Accuracy: 0.9375\n",
      "Step: 12380 Loss: 0.503416 Accuracy: 0.921875\n",
      "Step: 12390 Loss: 0.489589 Accuracy: 0.90625\n",
      "Step: 12400 Loss: 0.564437 Accuracy: 0.890625\n",
      "Test set accuracy: 0.814\n",
      "Step: 12410 Loss: 0.432566 Accuracy: 0.9375\n",
      "Step: 12420 Loss: 0.480064 Accuracy: 0.9375\n",
      "Step: 12430 Loss: 0.375672 Accuracy: 0.984375\n",
      "Step: 12440 Loss: 0.428878 Accuracy: 0.96875\n",
      "Step: 12450 Loss: 0.462557 Accuracy: 0.96875\n",
      "Step: 12460 Loss: 0.504367 Accuracy: 0.921875\n",
      "Step: 12470 Loss: 0.4111 Accuracy: 0.9375\n",
      "Switched CIFAR set to 2\n",
      "Step: 12480 Loss: 0.494527 Accuracy: 0.921875\n",
      "Step: 12490 Loss: 0.441742 Accuracy: 0.953125\n",
      "Step: 12500 Loss: 0.477039 Accuracy: 0.921875\n",
      "Test set accuracy: 0.86\n",
      "Step: 12510 Loss: 0.550568 Accuracy: 0.921875\n",
      "Step: 12520 Loss: 0.728244 Accuracy: 0.890625\n",
      "Step: 12530 Loss: 0.513057 Accuracy: 0.921875\n",
      "Step: 12540 Loss: 0.427664 Accuracy: 0.953125\n",
      "Step: 12550 Loss: 0.413293 Accuracy: 0.984375\n",
      "Step: 12560 Loss: 0.34825 Accuracy: 0.984375\n",
      "Step: 12570 Loss: 0.421678 Accuracy: 0.984375\n",
      "Step: 12580 Loss: 0.425066 Accuracy: 0.921875\n",
      "Step: 12590 Loss: 0.475919 Accuracy: 0.9375\n",
      "Step: 12600 Loss: 0.407815 Accuracy: 0.953125\n",
      "Test set accuracy: 0.806\n",
      "Step: 12610 Loss: 0.368839 Accuracy: 0.96875\n",
      "Step: 12620 Loss: 0.397761 Accuracy: 0.96875\n",
      "Step: 12630 Loss: 0.37998 Accuracy: 0.96875\n",
      "Switched CIFAR set to 3\n",
      "Step: 12640 Loss: 0.48289 Accuracy: 0.953125\n",
      "Step: 12650 Loss: 0.459084 Accuracy: 0.953125\n",
      "Step: 12660 Loss: 0.463255 Accuracy: 0.9375\n",
      "Step: 12670 Loss: 0.437077 Accuracy: 0.953125\n",
      "Step: 12680 Loss: 0.490494 Accuracy: 0.90625\n",
      "Step: 12690 Loss: 0.449465 Accuracy: 0.9375\n",
      "Step: 12700 Loss: 0.359759 Accuracy: 0.984375\n",
      "Test set accuracy: 0.848\n",
      "Step: 12710 Loss: 0.550251 Accuracy: 0.90625\n",
      "Step: 12720 Loss: 0.429881 Accuracy: 0.96875\n",
      "Step: 12730 Loss: 0.457556 Accuracy: 0.9375\n",
      "Step: 12740 Loss: 0.464321 Accuracy: 0.921875\n",
      "Step: 12750 Loss: 0.336601 Accuracy: 1.0\n",
      "Step: 12760 Loss: 0.35102 Accuracy: 1.0\n",
      "Step: 12770 Loss: 0.441604 Accuracy: 0.9375\n",
      "Step: 12780 Loss: 0.446371 Accuracy: 0.9375\n",
      "Step: 12790 Loss: 0.481237 Accuracy: 0.9375\n",
      "Switched CIFAR set to 4\n",
      "Step: 12800 Loss: 0.455617 Accuracy: 0.9375\n",
      "Test set accuracy: 0.842\n",
      "Step: 12810 Loss: 0.419129 Accuracy: 0.953125\n",
      "Step: 12820 Loss: 0.452729 Accuracy: 0.953125\n",
      "Step: 12830 Loss: 0.36729 Accuracy: 0.984375\n",
      "Step: 12840 Loss: 0.473074 Accuracy: 0.9375\n",
      "Step: 12850 Loss: 0.508631 Accuracy: 0.921875\n",
      "Step: 12860 Loss: 0.375494 Accuracy: 0.96875\n",
      "Step: 12870 Loss: 0.403039 Accuracy: 0.953125\n",
      "Step: 12880 Loss: 0.524858 Accuracy: 0.9375\n",
      "Step: 12890 Loss: 0.395095 Accuracy: 0.96875\n",
      "Step: 12900 Loss: 0.404498 Accuracy: 0.96875\n",
      "Test set accuracy: 0.82\n",
      "Step: 12910 Loss: 0.42246 Accuracy: 0.96875\n",
      "Step: 12920 Loss: 0.591565 Accuracy: 0.890625\n",
      "Step: 12930 Loss: 0.390167 Accuracy: 0.984375\n",
      "Step: 12940 Loss: 0.421606 Accuracy: 0.953125\n",
      "Switched CIFAR set to 5\n",
      "Step: 12950 Loss: 0.578646 Accuracy: 0.890625\n",
      "Step: 12960 Loss: 0.487594 Accuracy: 0.9375\n",
      "Step: 12970 Loss: 0.426343 Accuracy: 0.96875\n",
      "Step: 12980 Loss: 0.567335 Accuracy: 0.921875\n",
      "Step: 12990 Loss: 0.447434 Accuracy: 0.953125\n",
      "Step: 13000 Loss: 0.454101 Accuracy: 0.96875\n",
      "Test set accuracy: 0.83\n",
      "Step: 13010 Loss: 0.374042 Accuracy: 0.984375\n",
      "Step: 13020 Loss: 0.406733 Accuracy: 0.9375\n",
      "Step: 13030 Loss: 0.555773 Accuracy: 0.890625\n",
      "Step: 13040 Loss: 0.469299 Accuracy: 0.9375\n",
      "Step: 13050 Loss: 0.368377 Accuracy: 0.984375\n",
      "Step: 13060 Loss: 0.47215 Accuracy: 0.953125\n",
      "Step: 13070 Loss: 0.59367 Accuracy: 0.875\n",
      "Step: 13080 Loss: 0.622057 Accuracy: 0.859375\n",
      "Step: 13090 Loss: 0.339367 Accuracy: 0.984375\n",
      "Step: 13100 Loss: 0.438351 Accuracy: 0.9375\n",
      "Test set accuracy: 0.836\n",
      "Switched CIFAR set to 1\n",
      "Step: 13110 Loss: 0.443225 Accuracy: 0.96875\n",
      "Step: 13120 Loss: 0.512694 Accuracy: 0.953125\n",
      "Step: 13130 Loss: 0.530169 Accuracy: 0.9375\n",
      "Step: 13140 Loss: 0.438078 Accuracy: 0.9375\n",
      "Step: 13150 Loss: 0.454305 Accuracy: 0.953125\n",
      "Step: 13160 Loss: 0.455826 Accuracy: 0.9375\n",
      "Step: 13170 Loss: 0.444421 Accuracy: 0.953125\n",
      "Step: 13180 Loss: 0.39368 Accuracy: 0.96875\n",
      "Step: 13190 Loss: 0.35976 Accuracy: 0.96875\n",
      "Step: 13200 Loss: 0.435179 Accuracy: 0.953125\n",
      "Test set accuracy: 0.844\n",
      "Step: 13210 Loss: 0.448342 Accuracy: 0.9375\n",
      "Step: 13220 Loss: 0.501728 Accuracy: 0.9375\n",
      "Step: 13230 Loss: 0.496578 Accuracy: 0.921875\n",
      "Step: 13240 Loss: 0.469688 Accuracy: 0.90625\n",
      "Step: 13250 Loss: 0.480421 Accuracy: 0.90625\n",
      "Switched CIFAR set to 2\n",
      "Step: 13260 Loss: 0.412918 Accuracy: 0.96875\n",
      "Step: 13270 Loss: 0.409203 Accuracy: 0.96875\n",
      "Step: 13280 Loss: 0.510049 Accuracy: 0.921875\n",
      "Step: 13290 Loss: 0.497878 Accuracy: 0.90625\n",
      "Step: 13300 Loss: 0.420019 Accuracy: 0.953125\n",
      "Test set accuracy: 0.82\n",
      "Step: 13310 Loss: 0.368675 Accuracy: 0.984375\n",
      "Step: 13320 Loss: 0.501817 Accuracy: 0.9375\n",
      "Step: 13330 Loss: 0.404122 Accuracy: 0.953125\n",
      "Step: 13340 Loss: 0.474501 Accuracy: 0.921875\n",
      "Step: 13350 Loss: 0.601915 Accuracy: 0.890625\n",
      "Step: 13360 Loss: 0.420844 Accuracy: 0.9375\n",
      "Step: 13370 Loss: 0.530857 Accuracy: 0.921875\n",
      "Step: 13380 Loss: 0.489406 Accuracy: 0.9375\n",
      "Step: 13390 Loss: 0.409974 Accuracy: 0.96875\n",
      "Step: 13400 Loss: 0.466125 Accuracy: 0.96875\n",
      "Test set accuracy: 0.828\n",
      "Step: 13410 Loss: 0.386497 Accuracy: 0.96875\n",
      "Switched CIFAR set to 3\n",
      "Step: 13420 Loss: 0.376178 Accuracy: 0.96875\n",
      "Step: 13430 Loss: 0.429381 Accuracy: 0.953125\n",
      "Step: 13440 Loss: 0.574655 Accuracy: 0.890625\n",
      "Step: 13450 Loss: 0.458881 Accuracy: 0.96875\n",
      "Step: 13460 Loss: 0.331155 Accuracy: 1.0\n",
      "Step: 13470 Loss: 0.385341 Accuracy: 0.953125\n",
      "Step: 13480 Loss: 0.351615 Accuracy: 0.984375\n",
      "Step: 13490 Loss: 0.396828 Accuracy: 0.96875\n",
      "Step: 13500 Loss: 0.492325 Accuracy: 0.90625\n",
      "Test set accuracy: 0.828\n",
      "Step: 13510 Loss: 0.483299 Accuracy: 0.9375\n",
      "Step: 13520 Loss: 0.416504 Accuracy: 0.96875\n",
      "Step: 13530 Loss: 0.416773 Accuracy: 0.953125\n",
      "Step: 13540 Loss: 0.463629 Accuracy: 0.9375\n",
      "Step: 13550 Loss: 0.379283 Accuracy: 0.96875\n",
      "Step: 13560 Loss: 0.469982 Accuracy: 0.921875\n",
      "Step: 13570 Loss: 0.494479 Accuracy: 0.96875\n",
      "Switched CIFAR set to 4\n",
      "Step: 13580 Loss: 0.699928 Accuracy: 0.859375\n",
      "Step: 13590 Loss: 0.604749 Accuracy: 0.890625\n",
      "Step: 13600 Loss: 0.463669 Accuracy: 0.9375\n",
      "Test set accuracy: 0.814\n",
      "Step: 13610 Loss: 0.537425 Accuracy: 0.921875\n",
      "Step: 13620 Loss: 0.55103 Accuracy: 0.921875\n",
      "Step: 13630 Loss: 0.563934 Accuracy: 0.875\n",
      "Step: 13640 Loss: 0.39227 Accuracy: 0.9375\n",
      "Step: 13650 Loss: 0.399957 Accuracy: 0.96875\n",
      "Step: 13660 Loss: 0.577187 Accuracy: 0.90625\n",
      "Step: 13670 Loss: 0.391293 Accuracy: 0.984375\n",
      "Step: 13680 Loss: 0.431258 Accuracy: 0.953125\n",
      "Step: 13690 Loss: 0.476643 Accuracy: 0.9375\n",
      "Step: 13700 Loss: 0.477805 Accuracy: 0.9375\n",
      "Test set accuracy: 0.832\n",
      "Step: 13710 Loss: 0.357703 Accuracy: 0.953125\n",
      "Step: 13720 Loss: 0.44337 Accuracy: 0.921875\n",
      "Switched CIFAR set to 5\n",
      "Step: 13730 Loss: 0.526058 Accuracy: 0.921875\n",
      "Step: 13740 Loss: 0.445044 Accuracy: 0.953125\n",
      "Step: 13750 Loss: 0.422003 Accuracy: 0.953125\n",
      "Step: 13760 Loss: 0.498966 Accuracy: 0.921875\n",
      "Step: 13770 Loss: 0.36441 Accuracy: 0.96875\n",
      "Step: 13780 Loss: 0.455379 Accuracy: 0.9375\n",
      "Step: 13790 Loss: 0.429032 Accuracy: 0.9375\n",
      "Step: 13800 Loss: 0.430283 Accuracy: 0.96875\n",
      "Test set accuracy: 0.844\n",
      "Step: 13810 Loss: 0.49632 Accuracy: 0.921875\n",
      "Step: 13820 Loss: 0.388659 Accuracy: 0.96875\n",
      "Step: 13830 Loss: 0.444766 Accuracy: 0.953125\n",
      "Step: 13840 Loss: 0.36419 Accuracy: 0.984375\n",
      "Step: 13850 Loss: 0.497388 Accuracy: 0.9375\n",
      "Step: 13860 Loss: 0.347134 Accuracy: 0.984375\n",
      "Step: 13870 Loss: 0.410547 Accuracy: 0.96875\n",
      "Step: 13880 Loss: 0.555 Accuracy: 0.890625\n",
      "Switched CIFAR set to 1\n",
      "Step: 13890 Loss: 0.360411 Accuracy: 0.984375\n",
      "Step: 13900 Loss: 0.435167 Accuracy: 0.9375\n",
      "Test set accuracy: 0.84\n",
      "Step: 13910 Loss: 0.483908 Accuracy: 0.921875\n",
      "Step: 13920 Loss: 0.406999 Accuracy: 0.984375\n",
      "Step: 13930 Loss: 0.411289 Accuracy: 0.953125\n",
      "Step: 13940 Loss: 0.507056 Accuracy: 0.9375\n",
      "Step: 13950 Loss: 0.403213 Accuracy: 0.984375\n",
      "Step: 13960 Loss: 0.481548 Accuracy: 0.953125\n",
      "Step: 13970 Loss: 0.388835 Accuracy: 0.984375\n",
      "Step: 13980 Loss: 0.432912 Accuracy: 0.96875\n",
      "Step: 13990 Loss: 0.373041 Accuracy: 0.96875\n",
      "Step: 14000 Loss: 0.397975 Accuracy: 0.96875\n",
      "Test set accuracy: 0.842\n",
      "Step: 14010 Loss: 0.392109 Accuracy: 0.96875\n",
      "Step: 14020 Loss: 0.396166 Accuracy: 0.96875\n",
      "Step: 14030 Loss: 0.449201 Accuracy: 0.96875\n",
      "Switched CIFAR set to 2\n",
      "Step: 14040 Loss: 0.361996 Accuracy: 1.0\n",
      "Step: 14050 Loss: 0.448363 Accuracy: 0.9375\n",
      "Step: 14060 Loss: 0.394506 Accuracy: 0.96875\n",
      "Step: 14070 Loss: 0.414773 Accuracy: 0.9375\n",
      "Step: 14080 Loss: 0.503768 Accuracy: 0.9375\n",
      "Step: 14090 Loss: 0.380559 Accuracy: 0.984375\n",
      "Step: 14100 Loss: 0.437344 Accuracy: 0.953125\n",
      "Test set accuracy: 0.834\n",
      "Step: 14110 Loss: 0.355128 Accuracy: 0.984375\n",
      "Step: 14120 Loss: 0.384295 Accuracy: 0.96875\n",
      "Step: 14130 Loss: 0.397786 Accuracy: 0.96875\n",
      "Step: 14140 Loss: 0.442229 Accuracy: 0.953125\n",
      "Step: 14150 Loss: 0.39825 Accuracy: 0.984375\n",
      "Step: 14160 Loss: 0.430414 Accuracy: 0.96875\n",
      "Step: 14170 Loss: 0.445782 Accuracy: 0.953125\n",
      "Step: 14180 Loss: 0.363513 Accuracy: 0.96875\n",
      "Step: 14190 Loss: 0.476824 Accuracy: 0.953125\n",
      "Switched CIFAR set to 3\n",
      "Step: 14200 Loss: 0.401554 Accuracy: 0.953125\n",
      "Test set accuracy: 0.85\n",
      "Step: 14210 Loss: 0.391771 Accuracy: 0.9375\n",
      "Step: 14220 Loss: 0.388774 Accuracy: 0.96875\n",
      "Step: 14230 Loss: 0.501868 Accuracy: 0.921875\n",
      "Step: 14240 Loss: 0.478671 Accuracy: 0.96875\n",
      "Step: 14250 Loss: 0.333603 Accuracy: 0.984375\n",
      "Step: 14260 Loss: 0.449255 Accuracy: 0.953125\n",
      "Step: 14270 Loss: 0.42884 Accuracy: 0.9375\n",
      "Step: 14280 Loss: 0.379691 Accuracy: 1.0\n",
      "Step: 14290 Loss: 0.477872 Accuracy: 0.96875\n",
      "Step: 14300 Loss: 0.497717 Accuracy: 0.921875\n",
      "Test set accuracy: 0.848\n",
      "Step: 14310 Loss: 0.511459 Accuracy: 0.921875\n",
      "Step: 14320 Loss: 0.363755 Accuracy: 0.984375\n",
      "Step: 14330 Loss: 0.428437 Accuracy: 0.953125\n",
      "Step: 14340 Loss: 0.406547 Accuracy: 0.96875\n",
      "Step: 14350 Loss: 0.372914 Accuracy: 0.984375\n",
      "Switched CIFAR set to 4\n",
      "Step: 14360 Loss: 0.469063 Accuracy: 0.953125\n",
      "Step: 14370 Loss: 0.392904 Accuracy: 0.953125\n",
      "Step: 14380 Loss: 0.408818 Accuracy: 0.953125\n",
      "Step: 14390 Loss: 0.470453 Accuracy: 0.90625\n",
      "Step: 14400 Loss: 0.436207 Accuracy: 0.953125\n",
      "Test set accuracy: 0.842\n",
      "Step: 14410 Loss: 0.48673 Accuracy: 0.9375\n",
      "Step: 14420 Loss: 0.401118 Accuracy: 0.984375\n",
      "Step: 14430 Loss: 0.470714 Accuracy: 0.9375\n",
      "Step: 14440 Loss: 0.42812 Accuracy: 0.9375\n",
      "Step: 14450 Loss: 0.442701 Accuracy: 0.9375\n",
      "Step: 14460 Loss: 0.440002 Accuracy: 0.953125\n",
      "Step: 14470 Loss: 0.466404 Accuracy: 0.921875\n",
      "Step: 14480 Loss: 0.361986 Accuracy: 0.984375\n",
      "Step: 14490 Loss: 0.458817 Accuracy: 0.921875\n",
      "Step: 14500 Loss: 0.556802 Accuracy: 0.921875\n",
      "Test set accuracy: 0.784\n",
      "Switched CIFAR set to 5\n",
      "Step: 14510 Loss: 0.358572 Accuracy: 0.96875\n",
      "Step: 14520 Loss: 0.540278 Accuracy: 0.890625\n",
      "Step: 14530 Loss: 0.500881 Accuracy: 0.921875\n",
      "Step: 14540 Loss: 0.419969 Accuracy: 0.953125\n",
      "Step: 14550 Loss: 0.465972 Accuracy: 0.9375\n",
      "Step: 14560 Loss: 0.476851 Accuracy: 0.953125\n",
      "Step: 14570 Loss: 0.397713 Accuracy: 0.953125\n",
      "Step: 14580 Loss: 0.354963 Accuracy: 0.984375\n",
      "Step: 14590 Loss: 0.521797 Accuracy: 0.90625\n",
      "Step: 14600 Loss: 0.431248 Accuracy: 0.953125\n",
      "Test set accuracy: 0.832\n",
      "Step: 14610 Loss: 0.359681 Accuracy: 0.96875\n",
      "Step: 14620 Loss: 0.438555 Accuracy: 0.953125\n",
      "Step: 14630 Loss: 0.341684 Accuracy: 1.0\n",
      "Step: 14640 Loss: 0.418214 Accuracy: 0.953125\n",
      "Step: 14650 Loss: 0.373239 Accuracy: 0.96875\n",
      "Step: 14660 Loss: 0.546125 Accuracy: 0.9375\n",
      "Switched CIFAR set to 1\n",
      "Step: 14670 Loss: 0.422787 Accuracy: 0.96875\n",
      "Step: 14680 Loss: 0.494925 Accuracy: 0.921875\n",
      "Step: 14690 Loss: 0.441785 Accuracy: 0.953125\n",
      "Step: 14700 Loss: 0.381036 Accuracy: 0.984375\n",
      "Test set accuracy: 0.844\n",
      "Step: 14710 Loss: 0.455071 Accuracy: 0.953125\n",
      "Step: 14720 Loss: 0.416289 Accuracy: 0.953125\n",
      "Step: 14730 Loss: 0.49142 Accuracy: 0.921875\n",
      "Step: 14740 Loss: 0.524215 Accuracy: 0.921875\n",
      "Step: 14750 Loss: 0.435553 Accuracy: 0.96875\n",
      "Step: 14760 Loss: 0.360749 Accuracy: 0.984375\n",
      "Step: 14770 Loss: 0.546298 Accuracy: 0.90625\n",
      "Step: 14780 Loss: 0.384893 Accuracy: 0.953125\n",
      "Step: 14790 Loss: 0.432132 Accuracy: 0.953125\n",
      "Step: 14800 Loss: 0.487256 Accuracy: 0.953125\n",
      "Test set accuracy: 0.818\n",
      "Step: 14810 Loss: 0.59311 Accuracy: 0.90625\n",
      "Switched CIFAR set to 2\n",
      "Step: 14820 Loss: 0.528177 Accuracy: 0.921875\n",
      "Step: 14830 Loss: 0.502693 Accuracy: 0.9375\n",
      "Step: 14840 Loss: 0.584906 Accuracy: 0.90625\n",
      "Step: 14850 Loss: 0.415143 Accuracy: 0.953125\n",
      "Step: 14860 Loss: 0.528696 Accuracy: 0.90625\n",
      "Step: 14870 Loss: 0.384655 Accuracy: 0.96875\n",
      "Step: 14880 Loss: 0.420457 Accuracy: 0.96875\n",
      "Step: 14890 Loss: 0.460495 Accuracy: 0.953125\n",
      "Step: 14900 Loss: 0.48427 Accuracy: 0.9375\n",
      "Test set accuracy: 0.828\n",
      "Step: 14910 Loss: 0.393399 Accuracy: 0.953125\n",
      "Step: 14920 Loss: 0.421444 Accuracy: 0.953125\n",
      "Step: 14930 Loss: 0.419814 Accuracy: 0.953125\n",
      "Step: 14940 Loss: 0.422862 Accuracy: 0.953125\n",
      "Step: 14950 Loss: 0.449143 Accuracy: 0.9375\n",
      "Step: 14960 Loss: 0.636967 Accuracy: 0.890625\n",
      "Step: 14970 Loss: 0.473721 Accuracy: 0.953125\n",
      "Switched CIFAR set to 3\n",
      "Step: 14980 Loss: 0.420335 Accuracy: 0.953125\n",
      "Step: 14990 Loss: 0.362408 Accuracy: 0.984375\n",
      "Step: 15000 Loss: 0.548067 Accuracy: 0.9375\n",
      "Test set accuracy: 0.84\n",
      "Step: 15010 Loss: 0.44659 Accuracy: 0.96875\n",
      "Step: 15020 Loss: 0.520561 Accuracy: 0.9375\n",
      "Step: 15030 Loss: 0.424902 Accuracy: 0.96875\n",
      "Step: 15040 Loss: 0.438801 Accuracy: 0.9375\n",
      "Step: 15050 Loss: 0.392772 Accuracy: 0.953125\n",
      "Step: 15060 Loss: 0.442981 Accuracy: 0.9375\n",
      "Step: 15070 Loss: 0.405067 Accuracy: 0.953125\n",
      "Step: 15080 Loss: 0.334781 Accuracy: 1.0\n",
      "Step: 15090 Loss: 0.379731 Accuracy: 0.96875\n",
      "Step: 15100 Loss: 0.419389 Accuracy: 0.96875\n",
      "Test set accuracy: 0.812\n",
      "Step: 15110 Loss: 0.417321 Accuracy: 0.953125\n",
      "Step: 15120 Loss: 0.377822 Accuracy: 0.96875\n",
      "Step: 15130 Loss: 0.426437 Accuracy: 0.96875\n",
      "Switched CIFAR set to 4\n",
      "Step: 15140 Loss: 0.505526 Accuracy: 0.9375\n",
      "Step: 15150 Loss: 0.436001 Accuracy: 0.953125\n",
      "Step: 15160 Loss: 0.323345 Accuracy: 1.0\n",
      "Step: 15170 Loss: 0.492784 Accuracy: 0.9375\n",
      "Step: 15180 Loss: 0.519064 Accuracy: 0.921875\n",
      "Step: 15190 Loss: 0.45778 Accuracy: 0.953125\n",
      "Step: 15200 Loss: 0.414973 Accuracy: 0.953125\n",
      "Test set accuracy: 0.848\n",
      "Step: 15210 Loss: 0.386732 Accuracy: 0.96875\n",
      "Step: 15220 Loss: 0.447796 Accuracy: 0.953125\n",
      "Step: 15230 Loss: 0.596186 Accuracy: 0.859375\n",
      "Step: 15240 Loss: 0.388535 Accuracy: 0.96875\n",
      "Step: 15250 Loss: 0.35464 Accuracy: 0.96875\n",
      "Step: 15260 Loss: 0.383321 Accuracy: 0.984375\n",
      "Step: 15270 Loss: 0.388554 Accuracy: 0.984375\n",
      "Step: 15280 Loss: 0.310731 Accuracy: 1.0\n",
      "Switched CIFAR set to 5\n",
      "Step: 15290 Loss: 0.407132 Accuracy: 0.984375\n",
      "Step: 15300 Loss: 0.379811 Accuracy: 0.984375\n",
      "Test set accuracy: 0.834\n",
      "Step: 15310 Loss: 0.361351 Accuracy: 0.984375\n",
      "Step: 15320 Loss: 0.472216 Accuracy: 0.9375\n",
      "Step: 15330 Loss: 0.523341 Accuracy: 0.953125\n",
      "Step: 15340 Loss: 0.371003 Accuracy: 0.984375\n",
      "Step: 15350 Loss: 0.32637 Accuracy: 1.0\n",
      "Step: 15360 Loss: 0.404638 Accuracy: 0.96875\n",
      "Step: 15370 Loss: 0.422819 Accuracy: 0.9375\n",
      "Step: 15380 Loss: 0.349974 Accuracy: 1.0\n",
      "Step: 15390 Loss: 0.440623 Accuracy: 0.96875\n",
      "Step: 15400 Loss: 0.434062 Accuracy: 0.921875\n",
      "Test set accuracy: 0.822\n",
      "Step: 15410 Loss: 0.503392 Accuracy: 0.9375\n",
      "Step: 15420 Loss: 0.462927 Accuracy: 0.9375\n",
      "Step: 15430 Loss: 0.399599 Accuracy: 0.953125\n",
      "Step: 15440 Loss: 0.40133 Accuracy: 0.984375\n",
      "Switched CIFAR set to 1\n",
      "Step: 15450 Loss: 0.420891 Accuracy: 0.9375\n",
      "Step: 15460 Loss: 0.401797 Accuracy: 0.953125\n",
      "Step: 15470 Loss: 0.405885 Accuracy: 0.953125\n",
      "Step: 15480 Loss: 0.494982 Accuracy: 0.9375\n",
      "Step: 15490 Loss: 0.395982 Accuracy: 0.96875\n",
      "Step: 15500 Loss: 0.363001 Accuracy: 0.953125\n",
      "Test set accuracy: 0.808\n",
      "Step: 15510 Loss: 0.45472 Accuracy: 0.953125\n",
      "Step: 15520 Loss: 0.350242 Accuracy: 0.984375\n",
      "Step: 15530 Loss: 0.382299 Accuracy: 0.984375\n",
      "Step: 15540 Loss: 0.439431 Accuracy: 0.953125\n",
      "Step: 15550 Loss: 0.319412 Accuracy: 0.984375\n",
      "Step: 15560 Loss: 0.437684 Accuracy: 0.96875\n",
      "Step: 15570 Loss: 0.449977 Accuracy: 0.953125\n",
      "Step: 15580 Loss: 0.35572 Accuracy: 0.984375\n",
      "Step: 15590 Loss: 0.323009 Accuracy: 1.0\n",
      "Switched CIFAR set to 2\n",
      "Step: 15600 Loss: 0.539318 Accuracy: 0.921875\n",
      "Test set accuracy: 0.83\n",
      "Step: 15610 Loss: 0.395046 Accuracy: 0.96875\n",
      "Step: 15620 Loss: 0.371569 Accuracy: 0.96875\n",
      "Step: 15630 Loss: 0.359639 Accuracy: 0.984375\n",
      "Step: 15640 Loss: 0.467038 Accuracy: 0.9375\n",
      "Step: 15650 Loss: 0.327808 Accuracy: 1.0\n",
      "Step: 15660 Loss: 0.409935 Accuracy: 0.953125\n",
      "Step: 15670 Loss: 0.383596 Accuracy: 0.96875\n",
      "Step: 15680 Loss: 0.442115 Accuracy: 0.96875\n",
      "Step: 15690 Loss: 0.425785 Accuracy: 0.953125\n",
      "Step: 15700 Loss: 0.355173 Accuracy: 0.984375\n",
      "Test set accuracy: 0.846\n",
      "Step: 15710 Loss: 0.494817 Accuracy: 0.921875\n",
      "Step: 15720 Loss: 0.427795 Accuracy: 0.953125\n",
      "Step: 15730 Loss: 0.419732 Accuracy: 0.953125\n",
      "Step: 15740 Loss: 0.392893 Accuracy: 0.984375\n",
      "Step: 15750 Loss: 0.545763 Accuracy: 0.921875\n",
      "Switched CIFAR set to 3\n",
      "Step: 15760 Loss: 0.36435 Accuracy: 0.96875\n",
      "Step: 15770 Loss: 0.45188 Accuracy: 0.953125\n",
      "Step: 15780 Loss: 0.379091 Accuracy: 0.96875\n",
      "Step: 15790 Loss: 0.407436 Accuracy: 0.953125\n",
      "Step: 15800 Loss: 0.442316 Accuracy: 0.96875\n",
      "Test set accuracy: 0.818\n",
      "Step: 15810 Loss: 0.427908 Accuracy: 0.953125\n",
      "Step: 15820 Loss: 0.452668 Accuracy: 0.96875\n",
      "Step: 15830 Loss: 0.573093 Accuracy: 0.90625\n",
      "Step: 15840 Loss: 0.38878 Accuracy: 0.984375\n",
      "Step: 15850 Loss: 0.504123 Accuracy: 0.96875\n",
      "Step: 15860 Loss: 0.389081 Accuracy: 0.953125\n",
      "Step: 15870 Loss: 0.316385 Accuracy: 1.0\n",
      "Step: 15880 Loss: 0.446079 Accuracy: 0.9375\n",
      "Step: 15890 Loss: 0.374294 Accuracy: 0.984375\n",
      "Step: 15900 Loss: 0.437064 Accuracy: 0.953125\n",
      "Test set accuracy: 0.832\n",
      "Step: 15910 Loss: 0.433538 Accuracy: 0.953125\n",
      "Switched CIFAR set to 4\n",
      "Step: 15920 Loss: 0.400858 Accuracy: 0.96875\n",
      "Step: 15930 Loss: 0.44791 Accuracy: 0.953125\n",
      "Step: 15940 Loss: 0.435987 Accuracy: 0.921875\n",
      "Step: 15950 Loss: 0.396964 Accuracy: 0.96875\n",
      "Step: 15960 Loss: 0.477069 Accuracy: 0.96875\n",
      "Step: 15970 Loss: 0.362806 Accuracy: 1.0\n",
      "Step: 15980 Loss: 0.492869 Accuracy: 0.9375\n",
      "Step: 15990 Loss: 0.430302 Accuracy: 0.953125\n",
      "Step: 16000 Loss: 0.547565 Accuracy: 0.90625\n",
      "Test set accuracy: 0.828\n",
      "Step: 16010 Loss: 0.50202 Accuracy: 0.9375\n",
      "Step: 16020 Loss: 0.376299 Accuracy: 0.96875\n",
      "Step: 16030 Loss: 0.538633 Accuracy: 0.90625\n",
      "Step: 16040 Loss: 0.407965 Accuracy: 0.984375\n",
      "Step: 16050 Loss: 0.470335 Accuracy: 0.9375\n",
      "Step: 16060 Loss: 0.413211 Accuracy: 0.984375\n",
      "Switched CIFAR set to 5\n",
      "Step: 16070 Loss: 0.446706 Accuracy: 0.96875\n",
      "Step: 16080 Loss: 0.346698 Accuracy: 0.96875\n",
      "Step: 16090 Loss: 0.392622 Accuracy: 0.984375\n",
      "Step: 16100 Loss: 0.453868 Accuracy: 0.953125\n",
      "Test set accuracy: 0.81\n",
      "Step: 16110 Loss: 0.361364 Accuracy: 0.984375\n",
      "Step: 16120 Loss: 0.485907 Accuracy: 0.9375\n",
      "Step: 16130 Loss: 0.503657 Accuracy: 0.9375\n",
      "Step: 16140 Loss: 0.35232 Accuracy: 1.0\n",
      "Step: 16150 Loss: 0.421031 Accuracy: 0.953125\n",
      "Step: 16160 Loss: 0.500802 Accuracy: 0.9375\n",
      "Step: 16170 Loss: 0.345251 Accuracy: 0.984375\n",
      "Step: 16180 Loss: 0.466126 Accuracy: 0.921875\n",
      "Step: 16190 Loss: 0.423996 Accuracy: 0.96875\n",
      "Step: 16200 Loss: 0.356268 Accuracy: 0.984375\n",
      "Test set accuracy: 0.832\n",
      "Step: 16210 Loss: 0.393387 Accuracy: 0.96875\n",
      "Step: 16220 Loss: 0.551419 Accuracy: 0.953125\n",
      "Switched CIFAR set to 1\n",
      "Step: 16230 Loss: 0.353151 Accuracy: 0.984375\n",
      "Step: 16240 Loss: 0.383795 Accuracy: 0.96875\n",
      "Step: 16250 Loss: 0.402786 Accuracy: 0.96875\n",
      "Step: 16260 Loss: 0.382414 Accuracy: 0.96875\n",
      "Step: 16270 Loss: 0.382043 Accuracy: 0.96875\n",
      "Step: 16280 Loss: 0.356246 Accuracy: 1.0\n",
      "Step: 16290 Loss: 0.417847 Accuracy: 0.96875\n",
      "Step: 16300 Loss: 0.424496 Accuracy: 0.953125\n",
      "Test set accuracy: 0.788\n",
      "Step: 16310 Loss: 0.416056 Accuracy: 0.953125\n",
      "Step: 16320 Loss: 0.342311 Accuracy: 0.96875\n",
      "Step: 16330 Loss: 0.395692 Accuracy: 0.96875\n",
      "Step: 16340 Loss: 0.403651 Accuracy: 0.953125\n",
      "Step: 16350 Loss: 0.367589 Accuracy: 0.96875\n",
      "Step: 16360 Loss: 0.384401 Accuracy: 0.96875\n",
      "Step: 16370 Loss: 0.33934 Accuracy: 0.984375\n",
      "Switched CIFAR set to 2\n",
      "Step: 16380 Loss: 0.432786 Accuracy: 0.9375\n",
      "Step: 16390 Loss: 0.46162 Accuracy: 0.9375\n",
      "Step: 16400 Loss: 0.406713 Accuracy: 0.96875\n",
      "Test set accuracy: 0.86\n",
      "Step: 16410 Loss: 0.372607 Accuracy: 0.96875\n",
      "Step: 16420 Loss: 0.463987 Accuracy: 0.953125\n",
      "Step: 16430 Loss: 0.438423 Accuracy: 0.953125\n",
      "Step: 16440 Loss: 0.375032 Accuracy: 0.984375\n",
      "Step: 16450 Loss: 0.329887 Accuracy: 1.0\n",
      "Step: 16460 Loss: 0.410463 Accuracy: 0.984375\n",
      "Step: 16470 Loss: 0.453882 Accuracy: 0.953125\n",
      "Step: 16480 Loss: 0.388651 Accuracy: 0.96875\n",
      "Step: 16490 Loss: 0.353751 Accuracy: 0.96875\n",
      "Step: 16500 Loss: 0.458219 Accuracy: 0.953125\n",
      "Test set accuracy: 0.844\n",
      "Step: 16510 Loss: 0.316005 Accuracy: 1.0\n",
      "Step: 16520 Loss: 0.347388 Accuracy: 0.984375\n",
      "Step: 16530 Loss: 0.383383 Accuracy: 0.984375\n",
      "Switched CIFAR set to 3\n",
      "Step: 16540 Loss: 0.394566 Accuracy: 0.953125\n",
      "Step: 16550 Loss: 0.394514 Accuracy: 0.96875\n",
      "Step: 16560 Loss: 0.564155 Accuracy: 0.90625\n",
      "Step: 16570 Loss: 0.548696 Accuracy: 0.90625\n",
      "Step: 16580 Loss: 0.499079 Accuracy: 0.921875\n",
      "Step: 16590 Loss: 0.353951 Accuracy: 1.0\n",
      "Step: 16600 Loss: 0.475769 Accuracy: 0.9375\n",
      "Test set accuracy: 0.842\n",
      "Step: 16610 Loss: 0.395062 Accuracy: 0.953125\n",
      "Step: 16620 Loss: 0.343228 Accuracy: 0.984375\n",
      "Step: 16630 Loss: 0.329239 Accuracy: 1.0\n",
      "Step: 16640 Loss: 0.438344 Accuracy: 0.953125\n",
      "Step: 16650 Loss: 0.380321 Accuracy: 0.984375\n",
      "Step: 16660 Loss: 0.366005 Accuracy: 0.984375\n",
      "Step: 16670 Loss: 0.317604 Accuracy: 1.0\n",
      "Step: 16680 Loss: 0.399348 Accuracy: 0.984375\n",
      "Step: 16690 Loss: 0.495181 Accuracy: 0.953125\n",
      "Switched CIFAR set to 4\n",
      "Step: 16700 Loss: 0.403815 Accuracy: 0.96875\n",
      "Test set accuracy: 0.852\n",
      "Step: 16710 Loss: 0.387975 Accuracy: 0.96875\n",
      "Step: 16720 Loss: 0.341462 Accuracy: 0.96875\n",
      "Step: 16730 Loss: 0.413145 Accuracy: 0.984375\n",
      "Step: 16740 Loss: 0.370674 Accuracy: 0.984375\n",
      "Step: 16750 Loss: 0.395843 Accuracy: 0.953125\n",
      "Step: 16760 Loss: 0.446816 Accuracy: 0.96875\n",
      "Step: 16770 Loss: 0.473185 Accuracy: 0.953125\n",
      "Step: 16780 Loss: 0.360564 Accuracy: 0.984375\n",
      "Step: 16790 Loss: 0.382424 Accuracy: 0.984375\n",
      "Step: 16800 Loss: 0.396769 Accuracy: 0.96875\n",
      "Test set accuracy: 0.822\n",
      "Step: 16810 Loss: 0.510927 Accuracy: 0.90625\n",
      "Step: 16820 Loss: 0.388078 Accuracy: 0.96875\n",
      "Step: 16830 Loss: 0.370647 Accuracy: 0.984375\n",
      "Step: 16840 Loss: 0.367058 Accuracy: 0.984375\n",
      "Switched CIFAR set to 5\n",
      "Step: 16850 Loss: 0.331212 Accuracy: 1.0\n",
      "Step: 16860 Loss: 0.42611 Accuracy: 0.953125\n",
      "Step: 16870 Loss: 0.378368 Accuracy: 0.96875\n",
      "Step: 16880 Loss: 0.423226 Accuracy: 0.953125\n",
      "Step: 16890 Loss: 0.362488 Accuracy: 0.984375\n",
      "Step: 16900 Loss: 0.462159 Accuracy: 0.953125\n",
      "Test set accuracy: 0.796\n",
      "Step: 16910 Loss: 0.425016 Accuracy: 0.96875\n",
      "Step: 16920 Loss: 0.389465 Accuracy: 0.984375\n",
      "Step: 16930 Loss: 0.409599 Accuracy: 0.96875\n",
      "Step: 16940 Loss: 0.353663 Accuracy: 0.984375\n",
      "Step: 16950 Loss: 0.44915 Accuracy: 0.9375\n",
      "Step: 16960 Loss: 0.424079 Accuracy: 0.96875\n",
      "Step: 16970 Loss: 0.346676 Accuracy: 1.0\n",
      "Step: 16980 Loss: 0.378332 Accuracy: 0.953125\n",
      "Step: 16990 Loss: 0.356971 Accuracy: 0.984375\n",
      "Step: 17000 Loss: 0.387856 Accuracy: 0.9375\n",
      "Test set accuracy: 0.844\n",
      "Switched CIFAR set to 1\n",
      "Step: 17010 Loss: 0.465308 Accuracy: 0.96875\n",
      "Step: 17020 Loss: 0.518357 Accuracy: 0.9375\n",
      "Step: 17030 Loss: 0.364023 Accuracy: 0.96875\n",
      "Step: 17040 Loss: 0.367375 Accuracy: 0.96875\n",
      "Step: 17050 Loss: 0.451189 Accuracy: 0.953125\n",
      "Step: 17060 Loss: 0.421924 Accuracy: 0.9375\n",
      "Step: 17070 Loss: 0.393713 Accuracy: 0.96875\n",
      "Step: 17080 Loss: 0.354013 Accuracy: 0.984375\n",
      "Step: 17090 Loss: 0.401823 Accuracy: 0.984375\n",
      "Step: 17100 Loss: 0.359685 Accuracy: 0.96875\n",
      "Test set accuracy: 0.852\n",
      "Step: 17110 Loss: 0.353238 Accuracy: 0.984375\n",
      "Step: 17120 Loss: 0.369743 Accuracy: 0.984375\n",
      "Step: 17130 Loss: 0.321221 Accuracy: 1.0\n",
      "Step: 17140 Loss: 0.32734 Accuracy: 0.984375\n",
      "Step: 17150 Loss: 0.378038 Accuracy: 0.984375\n",
      "Switched CIFAR set to 2\n",
      "Step: 17160 Loss: 0.385972 Accuracy: 0.984375\n",
      "Step: 17170 Loss: 0.355629 Accuracy: 0.984375\n",
      "Step: 17180 Loss: 0.494114 Accuracy: 0.921875\n",
      "Step: 17190 Loss: 0.537692 Accuracy: 0.9375\n",
      "Step: 17200 Loss: 0.432931 Accuracy: 0.96875\n",
      "Test set accuracy: 0.832\n",
      "Step: 17210 Loss: 0.48057 Accuracy: 0.9375\n",
      "Step: 17220 Loss: 0.412645 Accuracy: 0.96875\n",
      "Step: 17230 Loss: 0.387588 Accuracy: 0.953125\n",
      "Step: 17240 Loss: 0.363398 Accuracy: 1.0\n",
      "Step: 17250 Loss: 0.420985 Accuracy: 0.96875\n",
      "Step: 17260 Loss: 0.359706 Accuracy: 0.96875\n",
      "Step: 17270 Loss: 0.400683 Accuracy: 0.96875\n",
      "Step: 17280 Loss: 0.418174 Accuracy: 0.953125\n",
      "Step: 17290 Loss: 0.343727 Accuracy: 1.0\n",
      "Step: 17300 Loss: 0.496276 Accuracy: 0.9375\n",
      "Test set accuracy: 0.83\n",
      "Step: 17310 Loss: 0.403038 Accuracy: 0.953125\n",
      "Switched CIFAR set to 3\n",
      "Step: 17320 Loss: 0.409005 Accuracy: 0.9375\n",
      "Step: 17330 Loss: 0.540894 Accuracy: 0.90625\n",
      "Step: 17340 Loss: 0.395528 Accuracy: 0.984375\n",
      "Step: 17350 Loss: 0.406576 Accuracy: 0.953125\n",
      "Step: 17360 Loss: 0.416467 Accuracy: 0.921875\n",
      "Step: 17370 Loss: 0.492166 Accuracy: 0.9375\n",
      "Step: 17380 Loss: 0.39188 Accuracy: 0.96875\n",
      "Step: 17390 Loss: 0.365034 Accuracy: 0.984375\n",
      "Step: 17400 Loss: 0.398652 Accuracy: 0.984375\n",
      "Test set accuracy: 0.84\n",
      "Step: 17410 Loss: 0.389346 Accuracy: 0.984375\n",
      "Step: 17420 Loss: 0.438698 Accuracy: 0.96875\n",
      "Step: 17430 Loss: 0.407995 Accuracy: 0.96875\n",
      "Step: 17440 Loss: 0.440351 Accuracy: 0.96875\n",
      "Step: 17450 Loss: 0.339444 Accuracy: 1.0\n",
      "Step: 17460 Loss: 0.387927 Accuracy: 0.96875\n",
      "Step: 17470 Loss: 0.335839 Accuracy: 0.96875\n",
      "Switched CIFAR set to 4\n",
      "Step: 17480 Loss: 0.473921 Accuracy: 0.96875\n",
      "Step: 17490 Loss: 0.332201 Accuracy: 1.0\n",
      "Step: 17500 Loss: 0.481192 Accuracy: 0.921875\n",
      "Test set accuracy: 0.804\n",
      "Step: 17510 Loss: 0.348755 Accuracy: 0.96875\n",
      "Step: 17520 Loss: 0.425732 Accuracy: 0.96875\n",
      "Step: 17530 Loss: 0.492248 Accuracy: 0.9375\n",
      "Step: 17540 Loss: 0.395069 Accuracy: 0.984375\n",
      "Step: 17550 Loss: 0.38083 Accuracy: 0.984375\n",
      "Step: 17560 Loss: 0.367744 Accuracy: 0.96875\n",
      "Step: 17570 Loss: 0.375359 Accuracy: 0.984375\n",
      "Step: 17580 Loss: 0.387542 Accuracy: 0.953125\n",
      "Step: 17590 Loss: 0.387433 Accuracy: 0.984375\n",
      "Step: 17600 Loss: 0.531897 Accuracy: 0.921875\n",
      "Test set accuracy: 0.852\n",
      "Step: 17610 Loss: 0.40454 Accuracy: 0.96875\n",
      "Step: 17620 Loss: 0.386957 Accuracy: 0.96875\n",
      "Switched CIFAR set to 5\n",
      "Step: 17630 Loss: 0.525085 Accuracy: 0.921875\n",
      "Step: 17640 Loss: 0.421802 Accuracy: 0.953125\n",
      "Step: 17650 Loss: 0.470715 Accuracy: 0.9375\n",
      "Step: 17660 Loss: 0.504895 Accuracy: 0.921875\n",
      "Step: 17670 Loss: 0.33909 Accuracy: 1.0\n",
      "Step: 17680 Loss: 0.402248 Accuracy: 0.953125\n",
      "Step: 17690 Loss: 0.324505 Accuracy: 0.984375\n",
      "Step: 17700 Loss: 0.382319 Accuracy: 0.984375\n",
      "Test set accuracy: 0.842\n",
      "Step: 17710 Loss: 0.366273 Accuracy: 0.96875\n",
      "Step: 17720 Loss: 0.313688 Accuracy: 1.0\n",
      "Step: 17730 Loss: 0.395906 Accuracy: 0.9375\n",
      "Step: 17740 Loss: 0.417755 Accuracy: 0.953125\n",
      "Step: 17750 Loss: 0.307922 Accuracy: 1.0\n",
      "Step: 17760 Loss: 0.429861 Accuracy: 0.96875\n",
      "Step: 17770 Loss: 0.364149 Accuracy: 0.984375\n",
      "Step: 17780 Loss: 0.399117 Accuracy: 0.96875\n",
      "Switched CIFAR set to 1\n",
      "Step: 17790 Loss: 0.416065 Accuracy: 0.9375\n",
      "Step: 17800 Loss: 0.360376 Accuracy: 0.984375\n",
      "Test set accuracy: 0.826\n",
      "Step: 17810 Loss: 0.331109 Accuracy: 0.984375\n",
      "Step: 17820 Loss: 0.441336 Accuracy: 0.953125\n",
      "Step: 17830 Loss: 0.35623 Accuracy: 0.953125\n",
      "Step: 17840 Loss: 0.417021 Accuracy: 0.984375\n",
      "Step: 17850 Loss: 0.362173 Accuracy: 0.984375\n",
      "Step: 17860 Loss: 0.509941 Accuracy: 0.921875\n",
      "Step: 17870 Loss: 0.359517 Accuracy: 1.0\n",
      "Step: 17880 Loss: 0.429713 Accuracy: 0.9375\n",
      "Step: 17890 Loss: 0.380062 Accuracy: 0.96875\n",
      "Step: 17900 Loss: 0.32998 Accuracy: 1.0\n",
      "Test set accuracy: 0.832\n",
      "Step: 17910 Loss: 0.362564 Accuracy: 0.984375\n",
      "Step: 17920 Loss: 0.397858 Accuracy: 0.984375\n",
      "Step: 17930 Loss: 0.39783 Accuracy: 0.96875\n",
      "Switched CIFAR set to 2\n",
      "Step: 17940 Loss: 0.379058 Accuracy: 0.984375\n",
      "Step: 17950 Loss: 0.513812 Accuracy: 0.953125\n",
      "Step: 17960 Loss: 0.497879 Accuracy: 0.90625\n",
      "Step: 17970 Loss: 0.358817 Accuracy: 0.984375\n",
      "Step: 17980 Loss: 0.464054 Accuracy: 0.96875\n",
      "Step: 17990 Loss: 0.401279 Accuracy: 0.96875\n",
      "Step: 18000 Loss: 0.408387 Accuracy: 0.953125\n",
      "Test set accuracy: 0.828\n",
      "Step: 18010 Loss: 0.345421 Accuracy: 0.96875\n",
      "Step: 18020 Loss: 0.424382 Accuracy: 0.953125\n",
      "Step: 18030 Loss: 0.384219 Accuracy: 0.96875\n",
      "Step: 18040 Loss: 0.362165 Accuracy: 0.96875\n",
      "Step: 18050 Loss: 0.40696 Accuracy: 0.984375\n",
      "Step: 18060 Loss: 0.296729 Accuracy: 1.0\n",
      "Step: 18070 Loss: 0.434328 Accuracy: 0.953125\n",
      "Step: 18080 Loss: 0.364642 Accuracy: 0.984375\n",
      "Step: 18090 Loss: 0.471286 Accuracy: 0.921875\n",
      "Switched CIFAR set to 3\n",
      "Step: 18100 Loss: 0.419292 Accuracy: 0.96875\n",
      "Test set accuracy: 0.838\n",
      "Step: 18110 Loss: 0.381119 Accuracy: 0.96875\n",
      "Step: 18120 Loss: 0.410075 Accuracy: 0.96875\n",
      "Step: 18130 Loss: 0.392036 Accuracy: 0.984375\n",
      "Step: 18140 Loss: 0.368476 Accuracy: 0.96875\n",
      "Step: 18150 Loss: 0.355186 Accuracy: 0.984375\n",
      "Step: 18160 Loss: 0.506036 Accuracy: 0.9375\n",
      "Step: 18170 Loss: 0.411594 Accuracy: 0.96875\n",
      "Step: 18180 Loss: 0.364851 Accuracy: 0.984375\n",
      "Step: 18190 Loss: 0.552964 Accuracy: 0.90625\n",
      "Step: 18200 Loss: 0.334696 Accuracy: 1.0\n",
      "Test set accuracy: 0.856\n",
      "Step: 18210 Loss: 0.409666 Accuracy: 0.96875\n",
      "Step: 18220 Loss: 0.350848 Accuracy: 0.984375\n",
      "Step: 18230 Loss: 0.42587 Accuracy: 0.953125\n",
      "Step: 18240 Loss: 0.37087 Accuracy: 0.96875\n",
      "Step: 18250 Loss: 0.345661 Accuracy: 1.0\n",
      "Switched CIFAR set to 4\n",
      "Step: 18260 Loss: 0.358606 Accuracy: 1.0\n",
      "Step: 18270 Loss: 0.369794 Accuracy: 0.984375\n",
      "Step: 18280 Loss: 0.368782 Accuracy: 0.96875\n",
      "Step: 18290 Loss: 0.42633 Accuracy: 0.921875\n",
      "Step: 18300 Loss: 0.421952 Accuracy: 0.96875\n",
      "Test set accuracy: 0.848\n",
      "Step: 18310 Loss: 0.468551 Accuracy: 0.96875\n",
      "Step: 18320 Loss: 0.331354 Accuracy: 1.0\n",
      "Step: 18330 Loss: 0.344888 Accuracy: 0.984375\n",
      "Step: 18340 Loss: 0.389425 Accuracy: 0.96875\n",
      "Step: 18350 Loss: 0.431584 Accuracy: 0.96875\n",
      "Step: 18360 Loss: 0.370139 Accuracy: 0.984375\n",
      "Step: 18370 Loss: 0.313911 Accuracy: 1.0\n",
      "Step: 18380 Loss: 0.431999 Accuracy: 0.953125\n",
      "Step: 18390 Loss: 0.461411 Accuracy: 0.9375\n",
      "Step: 18400 Loss: 0.351091 Accuracy: 0.96875\n",
      "Test set accuracy: 0.88\n",
      "Switched CIFAR set to 5\n",
      "Step: 18410 Loss: 0.360304 Accuracy: 0.984375\n",
      "Step: 18420 Loss: 0.598798 Accuracy: 0.890625\n",
      "Step: 18430 Loss: 0.380122 Accuracy: 0.984375\n",
      "Step: 18440 Loss: 0.35074 Accuracy: 0.96875\n",
      "Step: 18450 Loss: 0.303301 Accuracy: 1.0\n",
      "Step: 18460 Loss: 0.412293 Accuracy: 0.953125\n",
      "Step: 18470 Loss: 0.40864 Accuracy: 0.96875\n",
      "Step: 18480 Loss: 0.414277 Accuracy: 0.96875\n",
      "Step: 18490 Loss: 0.354464 Accuracy: 0.984375\n",
      "Step: 18500 Loss: 0.397194 Accuracy: 0.984375\n",
      "Test set accuracy: 0.854\n",
      "Step: 18510 Loss: 0.388474 Accuracy: 0.984375\n",
      "Step: 18520 Loss: 0.374365 Accuracy: 0.96875\n",
      "Step: 18530 Loss: 0.416673 Accuracy: 0.9375\n",
      "Step: 18540 Loss: 0.407871 Accuracy: 0.953125\n",
      "Step: 18550 Loss: 0.407321 Accuracy: 0.984375\n",
      "Step: 18560 Loss: 0.340123 Accuracy: 0.984375\n",
      "Switched CIFAR set to 1\n",
      "Step: 18570 Loss: 0.405615 Accuracy: 0.984375\n",
      "Step: 18580 Loss: 0.495116 Accuracy: 0.9375\n",
      "Step: 18590 Loss: 0.442908 Accuracy: 0.96875\n",
      "Step: 18600 Loss: 0.388298 Accuracy: 0.953125\n",
      "Test set accuracy: 0.862\n",
      "Step: 18610 Loss: 0.385458 Accuracy: 1.0\n",
      "Step: 18620 Loss: 0.462898 Accuracy: 0.9375\n",
      "Step: 18630 Loss: 0.33621 Accuracy: 0.984375\n",
      "Step: 18640 Loss: 0.423169 Accuracy: 0.96875\n",
      "Step: 18650 Loss: 0.325195 Accuracy: 1.0\n",
      "Step: 18660 Loss: 0.371469 Accuracy: 0.984375\n",
      "Step: 18670 Loss: 0.319059 Accuracy: 0.984375\n",
      "Step: 18680 Loss: 0.429195 Accuracy: 0.953125\n",
      "Step: 18690 Loss: 0.358977 Accuracy: 0.984375\n",
      "Step: 18700 Loss: 0.613275 Accuracy: 0.921875\n",
      "Test set accuracy: 0.86\n",
      "Step: 18710 Loss: 0.372442 Accuracy: 0.984375\n",
      "Switched CIFAR set to 2\n",
      "Step: 18720 Loss: 0.405344 Accuracy: 0.96875\n",
      "Step: 18730 Loss: 0.439837 Accuracy: 0.953125\n",
      "Step: 18740 Loss: 0.342921 Accuracy: 0.984375\n",
      "Step: 18750 Loss: 0.408758 Accuracy: 0.984375\n",
      "Step: 18760 Loss: 0.370823 Accuracy: 0.96875\n",
      "Step: 18770 Loss: 0.481853 Accuracy: 0.953125\n",
      "Step: 18780 Loss: 0.389527 Accuracy: 0.984375\n",
      "Step: 18790 Loss: 0.346065 Accuracy: 1.0\n",
      "Step: 18800 Loss: 0.367405 Accuracy: 0.96875\n",
      "Test set accuracy: 0.806\n",
      "Step: 18810 Loss: 0.394006 Accuracy: 0.96875\n",
      "Step: 18820 Loss: 0.467661 Accuracy: 0.953125\n",
      "Step: 18830 Loss: 0.403015 Accuracy: 0.953125\n",
      "Step: 18840 Loss: 0.366026 Accuracy: 0.984375\n",
      "Step: 18850 Loss: 0.325309 Accuracy: 1.0\n",
      "Step: 18860 Loss: 0.344517 Accuracy: 0.984375\n",
      "Step: 18870 Loss: 0.303503 Accuracy: 1.0\n",
      "Switched CIFAR set to 3\n",
      "Step: 18880 Loss: 0.33475 Accuracy: 1.0\n",
      "Step: 18890 Loss: 0.319077 Accuracy: 1.0\n",
      "Step: 18900 Loss: 0.352125 Accuracy: 0.984375\n",
      "Test set accuracy: 0.838\n",
      "Step: 18910 Loss: 0.367704 Accuracy: 0.984375\n",
      "Step: 18920 Loss: 0.497186 Accuracy: 0.9375\n",
      "Step: 18930 Loss: 0.350822 Accuracy: 0.984375\n",
      "Step: 18940 Loss: 0.440042 Accuracy: 0.9375\n",
      "Step: 18950 Loss: 0.429761 Accuracy: 0.96875\n",
      "Step: 18960 Loss: 0.369622 Accuracy: 0.984375\n",
      "Step: 18970 Loss: 0.453906 Accuracy: 0.984375\n",
      "Step: 18980 Loss: 0.380534 Accuracy: 0.96875\n",
      "Step: 18990 Loss: 0.344282 Accuracy: 0.984375\n",
      "Step: 19000 Loss: 0.323365 Accuracy: 1.0\n",
      "Test set accuracy: 0.874\n",
      "Step: 19010 Loss: 0.326182 Accuracy: 1.0\n",
      "Step: 19020 Loss: 0.436193 Accuracy: 1.0\n",
      "Step: 19030 Loss: 0.38248 Accuracy: 0.984375\n",
      "Switched CIFAR set to 4\n",
      "Step: 19040 Loss: 0.332841 Accuracy: 0.984375\n",
      "Step: 19050 Loss: 0.372999 Accuracy: 0.96875\n",
      "Step: 19060 Loss: 0.436157 Accuracy: 0.953125\n",
      "Step: 19070 Loss: 0.434792 Accuracy: 0.953125\n",
      "Step: 19080 Loss: 0.4944 Accuracy: 0.9375\n",
      "Step: 19090 Loss: 0.381441 Accuracy: 0.96875\n",
      "Step: 19100 Loss: 0.351503 Accuracy: 0.984375\n",
      "Test set accuracy: 0.842\n",
      "Step: 19110 Loss: 0.385229 Accuracy: 0.984375\n",
      "Step: 19120 Loss: 0.442772 Accuracy: 0.9375\n",
      "Step: 19130 Loss: 0.330768 Accuracy: 1.0\n",
      "Step: 19140 Loss: 0.308012 Accuracy: 1.0\n",
      "Step: 19150 Loss: 0.354027 Accuracy: 1.0\n",
      "Step: 19160 Loss: 0.322353 Accuracy: 1.0\n",
      "Step: 19170 Loss: 0.434072 Accuracy: 0.953125\n",
      "Step: 19180 Loss: 0.331183 Accuracy: 1.0\n",
      "Switched CIFAR set to 5\n",
      "Step: 19190 Loss: 0.34856 Accuracy: 0.984375\n",
      "Step: 19200 Loss: 0.398475 Accuracy: 0.96875\n",
      "Test set accuracy: 0.86\n",
      "Step: 19210 Loss: 0.418918 Accuracy: 0.96875\n",
      "Step: 19220 Loss: 0.407937 Accuracy: 0.984375\n",
      "Step: 19230 Loss: 0.445217 Accuracy: 0.9375\n",
      "Step: 19240 Loss: 0.357606 Accuracy: 0.984375\n",
      "Step: 19250 Loss: 0.339144 Accuracy: 0.96875\n",
      "Step: 19260 Loss: 0.38122 Accuracy: 0.984375\n",
      "Step: 19270 Loss: 0.384964 Accuracy: 0.953125\n",
      "Step: 19280 Loss: 0.374546 Accuracy: 0.984375\n",
      "Step: 19290 Loss: 0.337336 Accuracy: 1.0\n",
      "Step: 19300 Loss: 0.380318 Accuracy: 0.96875\n",
      "Test set accuracy: 0.844\n",
      "Step: 19310 Loss: 0.322335 Accuracy: 1.0\n",
      "Step: 19320 Loss: 0.372723 Accuracy: 0.984375\n",
      "Step: 19330 Loss: 0.407386 Accuracy: 0.96875\n",
      "Step: 19340 Loss: 0.376856 Accuracy: 0.984375\n",
      "Switched CIFAR set to 1\n",
      "Step: 19350 Loss: 0.365242 Accuracy: 0.984375\n",
      "Step: 19360 Loss: 0.415622 Accuracy: 0.96875\n",
      "Step: 19370 Loss: 0.556724 Accuracy: 0.90625\n",
      "Step: 19380 Loss: 0.404772 Accuracy: 0.984375\n",
      "Step: 19390 Loss: 0.329725 Accuracy: 1.0\n",
      "Step: 19400 Loss: 0.376917 Accuracy: 0.953125\n",
      "Test set accuracy: 0.852\n",
      "Step: 19410 Loss: 0.525922 Accuracy: 0.921875\n",
      "Step: 19420 Loss: 0.436655 Accuracy: 0.9375\n",
      "Step: 19430 Loss: 0.402528 Accuracy: 0.96875\n",
      "Step: 19440 Loss: 0.311952 Accuracy: 1.0\n",
      "Step: 19450 Loss: 0.423482 Accuracy: 0.96875\n",
      "Step: 19460 Loss: 0.346515 Accuracy: 1.0\n",
      "Step: 19470 Loss: 0.544375 Accuracy: 0.9375\n",
      "Step: 19480 Loss: 0.330047 Accuracy: 1.0\n",
      "Step: 19490 Loss: 0.397178 Accuracy: 0.984375\n",
      "Switched CIFAR set to 2\n",
      "Step: 19500 Loss: 0.345927 Accuracy: 0.984375\n",
      "Test set accuracy: 0.8\n",
      "Step: 19510 Loss: 0.429569 Accuracy: 0.921875\n",
      "Step: 19520 Loss: 0.474323 Accuracy: 0.953125\n",
      "Step: 19530 Loss: 0.478119 Accuracy: 0.9375\n",
      "Step: 19540 Loss: 0.384395 Accuracy: 0.953125\n",
      "Step: 19550 Loss: 0.457238 Accuracy: 0.96875\n",
      "Step: 19560 Loss: 0.381313 Accuracy: 0.984375\n",
      "Step: 19570 Loss: 0.361058 Accuracy: 0.953125\n",
      "Step: 19580 Loss: 0.381859 Accuracy: 0.96875\n",
      "Step: 19590 Loss: 0.330506 Accuracy: 1.0\n",
      "Step: 19600 Loss: 0.422548 Accuracy: 0.953125\n",
      "Test set accuracy: 0.868\n",
      "Step: 19610 Loss: 0.32653 Accuracy: 1.0\n",
      "Step: 19620 Loss: 0.43785 Accuracy: 0.96875\n",
      "Step: 19630 Loss: 0.302399 Accuracy: 1.0\n",
      "Step: 19640 Loss: 0.351333 Accuracy: 0.984375\n",
      "Step: 19650 Loss: 0.384756 Accuracy: 0.96875\n",
      "Switched CIFAR set to 3\n",
      "Step: 19660 Loss: 0.415212 Accuracy: 0.984375\n",
      "Step: 19670 Loss: 0.311483 Accuracy: 1.0\n",
      "Step: 19680 Loss: 0.338757 Accuracy: 0.984375\n",
      "Step: 19690 Loss: 0.4221 Accuracy: 0.96875\n",
      "Step: 19700 Loss: 0.370659 Accuracy: 0.96875\n",
      "Test set accuracy: 0.854\n",
      "Step: 19710 Loss: 0.383711 Accuracy: 0.96875\n",
      "Step: 19720 Loss: 0.412817 Accuracy: 0.953125\n",
      "Step: 19730 Loss: 0.438781 Accuracy: 0.953125\n",
      "Step: 19740 Loss: 0.421448 Accuracy: 0.984375\n",
      "Step: 19750 Loss: 0.459022 Accuracy: 0.921875\n",
      "Step: 19760 Loss: 0.421831 Accuracy: 0.96875\n",
      "Step: 19770 Loss: 0.417023 Accuracy: 0.96875\n",
      "Step: 19780 Loss: 0.373133 Accuracy: 0.96875\n",
      "Step: 19790 Loss: 0.45069 Accuracy: 0.96875\n",
      "Step: 19800 Loss: 0.384768 Accuracy: 0.984375\n",
      "Test set accuracy: 0.822\n",
      "Step: 19810 Loss: 0.344301 Accuracy: 0.984375\n",
      "Switched CIFAR set to 4\n",
      "Step: 19820 Loss: 0.41379 Accuracy: 0.96875\n",
      "Step: 19830 Loss: 0.382126 Accuracy: 0.96875\n",
      "Step: 19840 Loss: 0.394078 Accuracy: 0.96875\n",
      "Step: 19850 Loss: 0.371647 Accuracy: 0.96875\n",
      "Step: 19860 Loss: 0.323699 Accuracy: 0.984375\n",
      "Step: 19870 Loss: 0.400684 Accuracy: 0.984375\n",
      "Step: 19880 Loss: 0.532588 Accuracy: 0.953125\n",
      "Step: 19890 Loss: 0.447169 Accuracy: 0.9375\n",
      "Step: 19900 Loss: 0.343904 Accuracy: 1.0\n",
      "Test set accuracy: 0.848\n",
      "Step: 19910 Loss: 0.342675 Accuracy: 0.984375\n",
      "Step: 19920 Loss: 0.367667 Accuracy: 0.984375\n",
      "Step: 19930 Loss: 0.400164 Accuracy: 0.984375\n",
      "Step: 19940 Loss: 0.340485 Accuracy: 0.984375\n",
      "Step: 19950 Loss: 0.40275 Accuracy: 1.0\n",
      "Step: 19960 Loss: 0.514504 Accuracy: 0.953125\n",
      "Switched CIFAR set to 5\n",
      "Step: 19970 Loss: 0.41169 Accuracy: 0.96875\n",
      "Step: 19980 Loss: 0.402165 Accuracy: 0.984375\n",
      "Step: 19990 Loss: 0.488884 Accuracy: 0.953125\n",
      "Model saved in file: ./trainingmodels/model_resnet.ckpt\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 64\n",
    "currentCifar = 1\n",
    "total_steps = 20000\n",
    "l = []\n",
    "a = []\n",
    "aT = []\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "i = 0\n",
    "draw = range(10000)\n",
    "while i < total_steps:\n",
    "    if i % (10000/batch_size) != 0:\n",
    "        batch_index = np.random.choice(draw,size=batch_size,replace=False)\n",
    "    else:\n",
    "        draw = range(10000)\n",
    "        if currentCifar == 5:\n",
    "            currentCifar = 1\n",
    "            print \"Switched CIFAR set to \" + str(currentCifar)\n",
    "        else:\n",
    "            currentCifar = currentCifar + 1\n",
    "            print \"Switched CIFAR set to \" + str(currentCifar)\n",
    "        cifar = unpickle('./cifar10/data_batch_'+str(currentCifar))\n",
    "        batch_index = np.random.choice(draw,size=batch_size,replace=False)\n",
    "    x = ConvertImages(cifar['data'][batch_index])  \n",
    "    y = np.reshape(np.array(cifar['labels'])[batch_index],[batch_size,1])\n",
    "    _,lossA,yP,LO = sess.run([update,loss,output,label_oh],feed_dict={input_layer:x,label_layer:np.hstack(y)})\n",
    "    accuracy = np.sum(np.equal(np.hstack(y),np.argmax(yP,1)))/float(len(y))\n",
    "    l.append(lossA)\n",
    "    a.append(accuracy)\n",
    "    if i % 10 == 0: print \"Step: \" + str(i) + \" Loss: \" + str(lossA) + \" Accuracy: \" + str(accuracy)\n",
    "    if i % 100 == 0: \n",
    "        point = np.random.randint(0,10000-500)\n",
    "        xT = ConvertImages(cifarT['data'][point:point+500]) \n",
    "        yT = np.reshape(np.array(cifarT['labels'])[point:point+500],[500])\n",
    "        lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})\n",
    "        accuracy = np.sum(np.equal(yT,np.argmax(yP,1)))/float(len(yT))\n",
    "        aT.append(accuracy)\n",
    "        print \"Test set accuracy: \" + str(accuracy)\n",
    "    i+= 1\n",
    "save_path = saver.save(sess, \"./trainingmodels/DenseNet/model_densnet.ckpt\")\n",
    "print \"Model saved in file: \" + str(save_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb0c89ca10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7iIiSUjJXUQkCf0/mLbHr3D+a10AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcafd0e4910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l) #Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb062a5050>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFSyWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb062874d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a) #Plot training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcaaae90390>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKTUOabgrpdQ49P8BRjjwNfqM/s0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb062ac610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(aT) #Plot test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(aT) #Best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate test accuracy\n",
    "def CalcTestAccuracy(sess, data, labels, isTransformed):\n",
    "    \n",
    "    percentageList = np.zeros(10)\n",
    "    \n",
    "    yT = np.reshape(np.array(labels),[len(labels)])\n",
    "    if(isTransformed):\n",
    "        lossT,yP = sess.run([loss,output],feed_dict={input_layer:data.eval(),label_layer:yT})\n",
    "    else:\n",
    "        lossT,yP = sess.run([loss,output],feed_dict={input_layer:data,label_layer:yT})\n",
    "    \n",
    "    equal = np.equal(yT,np.argmax(yP,1))\n",
    "    accuracy = np.sum(equal)/float(len(yT))\n",
    "    print \"Test set accuracy: \" + str(accuracy)\n",
    "    \n",
    "    for i in range(0,len(yT)):\n",
    "        if equal[i] == False:\n",
    "            index = yT[i]\n",
    "            percentageList[index] += 1\n",
    "    \n",
    "    return accuracy, percentageList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RotateAndCalc(sess, images, labels):\n",
    "    #List with all accuracies\n",
    "    accList = []\n",
    "    prcList = np.zeros(len(labels))\n",
    "    \n",
    "    rotAngle = 10\n",
    "    curAngle = 10\n",
    "\n",
    "    #First run for original image\n",
    "    fAcc,fPrc = CalcTestAccuracy(sess, ConvertImages(images), labels, False)\n",
    "    accList.append(fAcc)\n",
    "    prcList = [x+y for x, y in zip(prcList, fPrc)]\n",
    "\n",
    "    #Rotate 10 degree\n",
    "    while curAngle < 360:\n",
    "        print \"Rotating degree \" + str(curAngle)\n",
    "        rotImages = tf.contrib.image.rotate(ConvertImages(images), np.radians(curAngle))\n",
    "\n",
    "        #Calc accuracy for the rotated images\n",
    "        sAcc, sPrc = CalcTestAccuracy(sess, rotImages, labels, True)\n",
    "        prcList = [x+y for x, y in zip(prcList, sPrc)]\n",
    "        accList.append(sAcc)\n",
    "\n",
    "        #Increment or curAngle with +rotAngle\n",
    "        curAngle += rotAngle\n",
    "       \n",
    "    #Get failures in percentage\n",
    "    prcSum = sum(prcList)\n",
    "    nPrcList = []\n",
    "    for numb in prcList:\n",
    "        nPrcList.append((numb / prcSum) * 100)\n",
    "    \n",
    "    return accList, nPrcList\n",
    "\n",
    "def GetLabelData(numbOfImg):\n",
    "    images = []\n",
    "    labels = []\n",
    "    currImg = 0\n",
    "    index = 0\n",
    "        \n",
    "    #Collect data for one specific label\n",
    "    while currImg < numbOfImg:\n",
    "        if cifarT['labels'][index] == label:\n",
    "            images.append(cifarT['data'][index])\n",
    "            labels.append(cifarT['labels'][index])\n",
    "            currImg += 1\n",
    "        index += 1\n",
    "    return images, labels\n",
    "\n",
    "def GetScrambleData(numbOfImg):\n",
    "    #Collect data across labels\n",
    "    images = cifarT['data'][0:numbOfImg]\n",
    "    labels = cifarT['labels'][0:numbOfImg]\n",
    "    return images, labels\n",
    "\n",
    "def PlotAcc(accList):\n",
    "    #Plot our accuracies\n",
    "    x = np.arange(0, 360, 10)\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy with rotating images')\n",
    "    plt.plot(x,accList)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./trainingmodels/DenseNet/model_densenet.ckpt\")\n",
    "    print \"Model restored.\"\n",
    "    \n",
    "    numbOfImg = 1000 #Number of images to be found\n",
    "    finalLabelList = [] #List with each labellist inside it\n",
    "    \n",
    "    #Scramble classification\n",
    "    '''\n",
    "    print \"Scramble classification\"\n",
    "    sImages, sLabels = GetScrambleData(1000)\n",
    "    sAcc, sPrc = RotateAndCalc(sess, sImages, sLabels)\n",
    "    print \"Percentage for failing classification: \"\n",
    "    print sPrc\n",
    "    print \"Total accuracy for the test set: \" + str(sAcc[0])\n",
    "    PlotAcc(sAcc)\n",
    "    '''\n",
    "    \n",
    "    #Label wise classification\n",
    "    print \"Label wise classification\"\n",
    "    for label in range(0,10): #Label value 0-9\n",
    "        print \"Classifying on label: \" + str(label)\n",
    "        images, labels = GetLabelData(numbOfImg)\n",
    "        lAcc,_ = RotateAndCalc(sess, images, labels)\n",
    "        finalLabelList.append(lAcc)\n",
    "        PlotAcc(lAcc)\n",
    "    \n",
    "    # Print all the label wise in same plot\n",
    "    Labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "    for i in range(len(finalLabelList)):\n",
    "        x = np.arange(0, 360, 10)\n",
    "        plt.xlabel('Degree')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy with rotating images')\n",
    "        plt.plot(x,finalLabelList[i], label=Labels[i])\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
